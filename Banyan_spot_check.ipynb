{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nView the README.md file for a full description of this code and how to use it.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load banyan_sigma.py\n",
    "\"\"\"\n",
    "View the README.md file for a full description of this code and how to use it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from banyan_sigma import banyan_sigma_solve_multivar, parabolic_cylinder_f5_mod, \\\n",
    "equatorial_galactic, matrix_set_product_A_single, matrix_vector_set_product_v_single, \\\n",
    "matrix_vector_set_product, scalar_set_product_multivariate, \\\n",
    "scalar_set_product_multivariate_variablemetric, matrix_set_inflation, equatorial_XYZ, equatorial_UVW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary packages\n",
    "import numpy as np #Numpy maths\n",
    "from scipy.special import erfc\n",
    "import os #Access to environment variables\n",
    "import pandas as pd #Pandas dataframes will be used to store BANYAN Sigma outputs\n",
    "from astropy.table import Table #Reading astro-formatted tables\n",
    "import warnings #Raise user-defined Python warnings\n",
    "import pdb #Debugging\n",
    "from scipy.stats import describe #Useful for debugging\n",
    "from scipy.misc import logsumexp #Useful to sum logarithms in a numerically stable way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A more user-friendly way to set break points\n",
    "stop = pdb.set_trace\n",
    "\n",
    "#A very small number used for numerical stability\n",
    "tiny_number = 1e-318\n",
    "\n",
    "#The total number of stars in the Besancon model within 300 pc to tranlate FPR to NFP\n",
    "total_besancon_objects = 7152397.0\n",
    "\n",
    "#Initiate some global constants\n",
    "kappa = 0.004743717361 #1 AU/yr to km/s divided by 1000.\n",
    "#For some reason \"from astropy import units as u; kappa=u.au.to(u.km)/u.year.to(u.s)\" is far less precise\n",
    "\n",
    "#J2000.0 Equatorial position of the Galactic North (b=90 degrees) from Carrol and Ostlie\n",
    "ra_pol = 192.8595\n",
    "dec_pol = 27.12825\n",
    "\n",
    "#J2000.0 Galactic latitude gb of the Celestial North pole (dec=90 degrees) from Carrol and Ostlie\n",
    "l_north = 122.932"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Galactic Coordinates matrix\n",
    "TGAL = (np.array([[-0.0548755604, -0.8734370902, -0.4838350155],\n",
    "\t[0.4941094279, -0.4448296300, 0.7469822445],\n",
    "\t[-0.8676661490,  -0.1980763734, 0.4559837762]]))\n",
    "\n",
    "#Initiate some secondary variables\n",
    "sin_dec_pol = np.sin(np.radians(dec_pol))\n",
    "cos_dec_pol = np.cos(np.radians(dec_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_data=None\n",
    "column_names=None\n",
    "hypotheses=None\n",
    "ln_priors=None\n",
    "ntargets_max=1e7\n",
    "ra=None\n",
    "dec=None\n",
    "pmra=None\n",
    "pmdec=None\n",
    "epmra=None\n",
    "epmdec=None\n",
    "dist=None\n",
    "edist=None\n",
    "rv=None\n",
    "erv=None\n",
    "psira=None\n",
    "psidec=None\n",
    "epsira=None\n",
    "epsidec=None\n",
    "plx=None\n",
    "eplx=None\n",
    "constraint_dist_per_hyp=None\n",
    "constraint_edist_per_hyp=None\n",
    "unit_priors=False\n",
    "lnp_only=False\n",
    "no_xyz=False\n",
    "use_rv=None\n",
    "use_dist=None\n",
    "use_plx=None\n",
    "use_psi=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.simbad import Simbad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.vizier import Vizier\n",
    "from astropy.coordinates import Angle\n",
    "from astropy.coordinates import SkyCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = Simbad.query_object('LkCa 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_s, dec_s = res1['RA'].data.data[0], res1['DEC'].data.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Vizier.query_region(ra_s + ' '+ dec_s, catalog=[\"I/345/gaia2\"], radius=Angle(600, \"arcsec\"))[\"I/345/gaia2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table masked=True length=50</i>\n",
       "<table id=\"table43398377600\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>RA_ICRS</th><th>e_RA_ICRS</th><th>DE_ICRS</th><th>e_DE_ICRS</th><th>Source</th><th>Plx</th><th>e_Plx</th><th>pmRA</th><th>e_pmRA</th><th>pmDE</th><th>e_pmDE</th><th>Dup</th><th>FG</th><th>e_FG</th><th>Gmag</th><th>e_Gmag</th><th>FBP</th><th>e_FBP</th><th>BPmag</th><th>e_BPmag</th><th>FRP</th><th>e_FRP</th><th>RPmag</th><th>e_RPmag</th><th>BP-RP</th><th>RV</th><th>e_RV</th><th>Teff</th><th>AG</th><th>E_BP-RP_</th><th>Rad</th><th>Lum</th></tr></thead>\n",
       "<thead><tr><th>deg</th><th>mas</th><th>deg</th><th>mas</th><th></th><th>mas</th><th>mas</th><th>mas / yr</th><th>mas / yr</th><th>mas / yr</th><th>mas / yr</th><th></th><th>e-/s</th><th>e-/s</th><th>mag</th><th>mag</th><th>e-/s</th><th>e-/s</th><th>mag</th><th>mag</th><th>e-/s</th><th>e-/s</th><th>mag</th><th>mag</th><th>mag</th><th>km / s</th><th>km / s</th><th>K</th><th>mag</th><th>mag</th><th>Rsun</th><th>Lsun</th></tr></thead>\n",
       "<thead><tr><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th><th>float64</th><th>float32</th><th>float64</th><th>float32</th><th>float64</th><th>float32</th><th>uint8</th><th>float32</th><th>float32</th><th>float64</th><th>float64</th><th>float32</th><th>float32</th><th>float64</th><th>float64</th><th>float32</th><th>float32</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float32</th><th>float64</th><th>float32</th><th>float32</th><th>float32</th><th>float64</th></tr></thead>\n",
       "<tr><td>64.20329373453</td><td>0.2186</td><td>27.98882518724</td><td>0.1246</td><td>164473165556977024</td><td>0.0659</td><td>0.2420</td><td>0.285</td><td>0.434</td><td>-1.290</td><td>0.338</td><td>0</td><td>632.2</td><td>1.329</td><td>18.6862</td><td>0.0023</td><td>210.5</td><td>11.3</td><td>19.5433</td><td>0.0583</td><td>820.8</td><td>13.09</td><td>17.4763</td><td>0.0173</td><td>2.0670</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.20363930008</td><td>0.8738</td><td>27.98856164673</td><td>0.4991</td><td>164473165555228032</td><td>1.1949</td><td>0.8322</td><td>-0.093</td><td>1.902</td><td>-3.830</td><td>1.680</td><td>0</td><td>163.7</td><td>0.847</td><td>20.1535</td><td>0.0056</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.20352501015</td><td>0.2162</td><td>27.99482685215</td><td>0.1222</td><td>164473161260949504</td><td>0.9283</td><td>0.2398</td><td>5.242</td><td>0.436</td><td>-4.137</td><td>0.336</td><td>0</td><td>724.4</td><td>1.51</td><td>18.5384</td><td>0.0023</td><td>197.7</td><td>11.49</td><td>19.6112</td><td>0.0631</td><td>919</td><td>11.45</td><td>17.3537</td><td>0.0135</td><td>2.2575</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.21244737604</td><td>0.3914</td><td>27.98464843223</td><td>0.2210</td><td>164473161261051392</td><td>0.2689</td><td>0.4331</td><td>2.011</td><td>0.764</td><td>-5.012</td><td>0.598</td><td>0</td><td>292.1</td><td>0.9609</td><td>19.5246</td><td>0.0036</td><td>79.19</td><td>8.479</td><td>20.6048</td><td>0.1163</td><td>399.8</td><td>9.446</td><td>18.2573</td><td>0.0257</td><td>2.3475</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.19161746732</td><td>5.6411</td><td>27.98205367766</td><td>3.1088</td><td>164473131196126464</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>0</td><td>59.94</td><td>2.466</td><td>21.2440</td><td>0.0447</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.19199089270</td><td>0.3273</td><td>27.97806299371</td><td>0.1759</td><td>164473126901208064</td><td>0.5060</td><td>0.3702</td><td>1.644</td><td>0.669</td><td>-1.114</td><td>0.483</td><td>0</td><td>356.1</td><td>0.9943</td><td>19.3094</td><td>0.0030</td><td>109.6</td><td>6.358</td><td>20.2515</td><td>0.0630</td><td>408.9</td><td>9.315</td><td>18.2329</td><td>0.0247</td><td>2.0186</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.22571946355</td><td>0.1149</td><td>27.99873152541</td><td>0.0623</td><td>164473577873835520</td><td>0.6224</td><td>0.1267</td><td>-0.028</td><td>0.237</td><td>-0.211</td><td>0.179</td><td>0</td><td>2111</td><td>1.831</td><td>17.3772</td><td>0.0009</td><td>551.7</td><td>7.74</td><td>18.4972</td><td>0.0152</td><td>2443</td><td>11.53</td><td>16.2919</td><td>0.0051</td><td>2.2052</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.21545180574</td><td>0.2001</td><td>27.99593650430</td><td>0.1090</td><td>164473543514097792</td><td>0.0886</td><td>0.2232</td><td>-3.744</td><td>0.396</td><td>-2.778</td><td>0.300</td><td>0</td><td>780</td><td>1.241</td><td>18.4581</td><td>0.0017</td><td>206.9</td><td>8.379</td><td>19.5620</td><td>0.0440</td><td>930.5</td><td>7.857</td><td>17.3401</td><td>0.0092</td><td>2.2219</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.24587131618</td><td>0.7642</td><td>28.00925602088</td><td>0.4112</td><td>164473607937651840</td><td>1.2631</td><td>0.8554</td><td>1.347</td><td>1.572</td><td>-2.556</td><td>1.103</td><td>0</td><td>146.6</td><td>0.8008</td><td>20.2732</td><td>0.0059</td><td>59.11</td><td>8.077</td><td>20.9222</td><td>0.1484</td><td>188.1</td><td>9.772</td><td>19.0757</td><td>0.0564</td><td>1.8465</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>64.08507809933</td><td>10.2038</td><td>28.01530376517</td><td>3.7043</td><td>164475845615676544</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>0</td><td>58.58</td><td>2.042</td><td>21.2689</td><td>0.0379</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.06364572320</td><td>0.2784</td><td>28.01473284942</td><td>0.1618</td><td>164475875680283136</td><td>0.9367</td><td>0.2978</td><td>1.429</td><td>0.596</td><td>0.048</td><td>0.475</td><td>0</td><td>439.9</td><td>1.093</td><td>19.0800</td><td>0.0027</td><td>108.4</td><td>8.65</td><td>20.2633</td><td>0.0866</td><td>549.9</td><td>8.494</td><td>17.9113</td><td>0.0168</td><td>2.3521</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.07263772969</td><td>0.1875</td><td>28.01893866428</td><td>0.1070</td><td>164475910040021760</td><td>0.4640</td><td>0.2005</td><td>1.829</td><td>0.405</td><td>-3.616</td><td>0.313</td><td>0</td><td>818.6</td><td>1.321</td><td>18.4057</td><td>0.0018</td><td>209.3</td><td>6.778</td><td>19.5497</td><td>0.0352</td><td>909.2</td><td>9.026</td><td>17.3653</td><td>0.0108</td><td>2.1843</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.06433764387</td><td>0.3433</td><td>28.01615154484</td><td>0.2035</td><td>164475879976311040</td><td>0.5557</td><td>0.3720</td><td>2.065</td><td>0.730</td><td>-8.230</td><td>0.591</td><td>0</td><td>287.9</td><td>0.8907</td><td>19.5404</td><td>0.0034</td><td>65.19</td><td>7.206</td><td>20.8160</td><td>0.1200</td><td>386.8</td><td>11.73</td><td>18.2933</td><td>0.0329</td><td>2.5227</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.07442759405</td><td>0.0699</td><td>28.03051010560</td><td>0.0395</td><td>164475914336048000</td><td>-0.0376</td><td>0.0728</td><td>0.812</td><td>0.160</td><td>-1.075</td><td>0.123</td><td>0</td><td>4200</td><td>2.797</td><td>16.6304</td><td>0.0007</td><td>1070</td><td>9.82</td><td>17.7784</td><td>0.0100</td><td>4794</td><td>13.73</td><td>15.5602</td><td>0.0031</td><td>2.2182</td><td>--</td><td>--</td><td>3589.75</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.08591876476</td><td>0.1076</td><td>28.02151787685</td><td>0.0617</td><td>164475845616571392</td><td>0.5168</td><td>0.1131</td><td>4.853</td><td>0.242</td><td>-0.180</td><td>0.191</td><td>0</td><td>1906</td><td>1.795</td><td>17.4880</td><td>0.0010</td><td>489.6</td><td>8.28</td><td>18.6267</td><td>0.0184</td><td>2155</td><td>9.664</td><td>16.4282</td><td>0.0049</td><td>2.1985</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.07793285655</td><td>9.8692</td><td>28.02298993083</td><td>4.1937</td><td>164475910040534272</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>0</td><td>68.03</td><td>1.477</td><td>21.1067</td><td>0.0236</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.07627665056</td><td>0.2149</td><td>28.01642101800</td><td>0.1206</td><td>164475841320646272</td><td>2.5227</td><td>0.2273</td><td>-0.760</td><td>0.464</td><td>-6.635</td><td>0.359</td><td>0</td><td>668.9</td><td>1.207</td><td>18.6249</td><td>0.0020</td><td>100.1</td><td>7.405</td><td>20.3506</td><td>0.0803</td><td>948.9</td><td>9.785</td><td>17.3189</td><td>0.0112</td><td>3.0317</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.06330617922</td><td>0.4295</td><td>28.01618408763</td><td>0.2545</td><td>164475875680516736</td><td>0.1471</td><td>0.4607</td><td>-1.166</td><td>0.913</td><td>-2.053</td><td>0.742</td><td>0</td><td>253.1</td><td>0.8673</td><td>19.6800</td><td>0.0037</td><td>59.99</td><td>7.424</td><td>20.9062</td><td>0.1344</td><td>329.3</td><td>8.843</td><td>18.4678</td><td>0.0292</td><td>2.4384</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>64.06959570118</td><td>0.9416</td><td>28.03382664864</td><td>0.6346</td><td>164475914334930560</td><td>0.4308</td><td>0.9127</td><td>-1.832</td><td>2.899</td><td>-0.419</td><td>2.624</td><td>0</td><td>101.7</td><td>0.7512</td><td>20.6697</td><td>0.0080</td><td>35.5</td><td>4.088</td><td>21.4759</td><td>0.1251</td><td>135.5</td><td>10.76</td><td>19.4324</td><td>0.0862</td><td>2.0435</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table masked=True length=50>\n",
       "    RA_ICRS     e_RA_ICRS     DE_ICRS     e_DE_ICRS ... E_BP-RP_   Rad     Lum  \n",
       "      deg          mas          deg          mas    ...   mag      Rsun    Lsun \n",
       "    float64      float64      float64      float64  ... float32  float32 float64\n",
       "--------------- --------- --------------- --------- ... -------- ------- -------\n",
       " 64.20329373453    0.2186  27.98882518724    0.1246 ...       --      --      --\n",
       " 64.20363930008    0.8738  27.98856164673    0.4991 ...       --      --      --\n",
       " 64.20352501015    0.2162  27.99482685215    0.1222 ...       --      --      --\n",
       " 64.21244737604    0.3914  27.98464843223    0.2210 ...       --      --      --\n",
       " 64.19161746732    5.6411  27.98205367766    3.1088 ...       --      --      --\n",
       " 64.19199089270    0.3273  27.97806299371    0.1759 ...       --      --      --\n",
       " 64.22571946355    0.1149  27.99873152541    0.0623 ...       --      --      --\n",
       " 64.21545180574    0.2001  27.99593650430    0.1090 ...       --      --      --\n",
       " 64.24587131618    0.7642  28.00925602088    0.4112 ...       --      --      --\n",
       "            ...       ...             ...       ... ...      ...     ...     ...\n",
       " 64.08507809933   10.2038  28.01530376517    3.7043 ...       --      --      --\n",
       " 64.06364572320    0.2784  28.01473284942    0.1618 ...       --      --      --\n",
       " 64.07263772969    0.1875  28.01893866428    0.1070 ...       --      --      --\n",
       " 64.06433764387    0.3433  28.01615154484    0.2035 ...       --      --      --\n",
       " 64.07442759405    0.0699  28.03051010560    0.0395 ...       --      --      --\n",
       " 64.08591876476    0.1076  28.02151787685    0.0617 ...       --      --      --\n",
       " 64.07793285655    9.8692  28.02298993083    4.1937 ...       --      --      --\n",
       " 64.07627665056    0.2149  28.01642101800    0.1206 ...       --      --      --\n",
       " 64.06330617922    0.4295  28.01618408763    0.2545 ...       --      --      --\n",
       " 64.06959570118    0.9416  28.03382664864    0.6346 ...       --      --      --"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ra=0.035320833333333336\n",
    "#dec=36.58595833333334\n",
    "#pmra=-6.88\n",
    "#epmra=0.5799999833106995\n",
    "#pmdec=8.57\n",
    "#epmdec=1.0399999618530273\n",
    "#plx=2.38\n",
    "#eplx=0.9300000071525574\n",
    "#rv=float(np.NaN)\n",
    "#erv=float(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepcols = ['RA_ICRS', 'DE_ICRS', 'Plx','e_Plx','pmRA','e_pmRA','pmDE','e_pmDE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[keepcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'RA_ICRS':'RA', 'DE_ICRS':'DEC', 'Plx':'PLX','e_Plx':'EPLX',\n",
    "               'pmRA':'PMRA','e_pmRA':'EPMRA','pmDE':'PMDEC','e_pmDE':'EPMDEC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in zip(rename_dict.keys(), rename_dict.values()):\n",
    "    result.rename_column(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_data = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#banyan_sigma(ra=0.035320833333333336,dec=36.58595833333334,\n",
    "#                     pmra=-6.88,epmra=0.5799999833106995,\n",
    "#                     pmdec=8.57,epmdec=1.0399999618530273,\n",
    "#                     plx=2.38, eplx=0.9300000071525574, \n",
    "#                     rv=float(np.NaN), erv=float(np.NaN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.remove_rows(result['PLX'].mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table masked=True length=37</i>\n",
       "<table id=\"table43605764416\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>RA</th><th>DEC</th><th>PLX</th><th>EPLX</th><th>PMRA</th><th>EPMRA</th><th>PMDEC</th><th>EPMDEC</th></tr></thead>\n",
       "<thead><tr><th>deg</th><th>deg</th><th>mas</th><th>mas</th><th>mas / yr</th><th>mas / yr</th><th>mas / yr</th><th>mas / yr</th></tr></thead>\n",
       "<thead><tr><th>float64</th><th>float64</th><th>float64</th><th>float32</th><th>float64</th><th>float32</th><th>float64</th><th>float32</th></tr></thead>\n",
       "<tr><td>64.20329373453</td><td>27.98882518724</td><td>0.0659</td><td>0.2420</td><td>0.285</td><td>0.434</td><td>-1.290</td><td>0.338</td></tr>\n",
       "<tr><td>64.20363930008</td><td>27.98856164673</td><td>1.1949</td><td>0.8322</td><td>-0.093</td><td>1.902</td><td>-3.830</td><td>1.680</td></tr>\n",
       "<tr><td>64.20352501015</td><td>27.99482685215</td><td>0.9283</td><td>0.2398</td><td>5.242</td><td>0.436</td><td>-4.137</td><td>0.336</td></tr>\n",
       "<tr><td>64.21244737604</td><td>27.98464843223</td><td>0.2689</td><td>0.4331</td><td>2.011</td><td>0.764</td><td>-5.012</td><td>0.598</td></tr>\n",
       "<tr><td>64.19199089270</td><td>27.97806299371</td><td>0.5060</td><td>0.3702</td><td>1.644</td><td>0.669</td><td>-1.114</td><td>0.483</td></tr>\n",
       "<tr><td>64.22571946355</td><td>27.99873152541</td><td>0.6224</td><td>0.1267</td><td>-0.028</td><td>0.237</td><td>-0.211</td><td>0.179</td></tr>\n",
       "<tr><td>64.21545180574</td><td>27.99593650430</td><td>0.0886</td><td>0.2232</td><td>-3.744</td><td>0.396</td><td>-2.778</td><td>0.300</td></tr>\n",
       "<tr><td>64.24587131618</td><td>28.00925602088</td><td>1.2631</td><td>0.8554</td><td>1.347</td><td>1.572</td><td>-2.556</td><td>1.103</td></tr>\n",
       "<tr><td>64.24301037453</td><td>28.01273265368</td><td>0.1361</td><td>0.4494</td><td>3.046</td><td>0.798</td><td>-3.693</td><td>0.612</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>64.04322546888</td><td>28.00425337858</td><td>5.1564</td><td>0.1926</td><td>43.062</td><td>0.396</td><td>-1.710</td><td>0.310</td></tr>\n",
       "<tr><td>64.05847803865</td><td>27.98261277742</td><td>0.5845</td><td>0.7790</td><td>2.647</td><td>2.103</td><td>-0.680</td><td>1.958</td></tr>\n",
       "<tr><td>64.06364572320</td><td>28.01473284942</td><td>0.9367</td><td>0.2978</td><td>1.429</td><td>0.596</td><td>0.048</td><td>0.475</td></tr>\n",
       "<tr><td>64.07263772969</td><td>28.01893866428</td><td>0.4640</td><td>0.2005</td><td>1.829</td><td>0.405</td><td>-3.616</td><td>0.313</td></tr>\n",
       "<tr><td>64.06433764387</td><td>28.01615154484</td><td>0.5557</td><td>0.3720</td><td>2.065</td><td>0.730</td><td>-8.230</td><td>0.591</td></tr>\n",
       "<tr><td>64.07442759405</td><td>28.03051010560</td><td>-0.0376</td><td>0.0728</td><td>0.812</td><td>0.160</td><td>-1.075</td><td>0.123</td></tr>\n",
       "<tr><td>64.08591876476</td><td>28.02151787685</td><td>0.5168</td><td>0.1131</td><td>4.853</td><td>0.242</td><td>-0.180</td><td>0.191</td></tr>\n",
       "<tr><td>64.07627665056</td><td>28.01642101800</td><td>2.5227</td><td>0.2273</td><td>-0.760</td><td>0.464</td><td>-6.635</td><td>0.359</td></tr>\n",
       "<tr><td>64.06330617922</td><td>28.01618408763</td><td>0.1471</td><td>0.4607</td><td>-1.166</td><td>0.913</td><td>-2.053</td><td>0.742</td></tr>\n",
       "<tr><td>64.06959570118</td><td>28.03382664864</td><td>0.4308</td><td>0.9127</td><td>-1.832</td><td>2.899</td><td>-0.419</td><td>2.624</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table masked=True length=37>\n",
       "       RA             DEC          PLX     ...  EPMRA     PMDEC    EPMDEC \n",
       "      deg             deg          mas     ... mas / yr  mas / yr mas / yr\n",
       "    float64         float64      float64   ... float32   float64  float32 \n",
       "--------------- --------------- ---------- ... -------- --------- --------\n",
       " 64.20329373453  27.98882518724     0.0659 ...    0.434    -1.290    0.338\n",
       " 64.20363930008  27.98856164673     1.1949 ...    1.902    -3.830    1.680\n",
       " 64.20352501015  27.99482685215     0.9283 ...    0.436    -4.137    0.336\n",
       " 64.21244737604  27.98464843223     0.2689 ...    0.764    -5.012    0.598\n",
       " 64.19199089270  27.97806299371     0.5060 ...    0.669    -1.114    0.483\n",
       " 64.22571946355  27.99873152541     0.6224 ...    0.237    -0.211    0.179\n",
       " 64.21545180574  27.99593650430     0.0886 ...    0.396    -2.778    0.300\n",
       " 64.24587131618  28.00925602088     1.2631 ...    1.572    -2.556    1.103\n",
       " 64.24301037453  28.01273265368     0.1361 ...    0.798    -3.693    0.612\n",
       "            ...             ...        ... ...      ...       ...      ...\n",
       " 64.04322546888  28.00425337858     5.1564 ...    0.396    -1.710    0.310\n",
       " 64.05847803865  27.98261277742     0.5845 ...    2.103    -0.680    1.958\n",
       " 64.06364572320  28.01473284942     0.9367 ...    0.596     0.048    0.475\n",
       " 64.07263772969  28.01893866428     0.4640 ...    0.405    -3.616    0.313\n",
       " 64.06433764387  28.01615154484     0.5557 ...    0.730    -8.230    0.591\n",
       " 64.07442759405  28.03051010560    -0.0376 ...    0.160    -1.075    0.123\n",
       " 64.08591876476  28.02151787685     0.5168 ...    0.242    -0.180    0.191\n",
       " 64.07627665056  28.01642101800     2.5227 ...    0.464    -6.635    0.359\n",
       " 64.06330617922  28.01618408763     0.1471 ...    0.913    -2.053    0.742\n",
       " 64.06959570118  28.03382664864     0.4308 ...    2.899    -0.419    2.624"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = '/Users/obsidian/GitHub/banyan_sigma/banyan_sigma.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/obsidian/GitHub/banyan_sigma/banyan_sigma.py:45: UserWarning: Parallaxes (PLX) were not read from the input data, because the PLX key was not included in the column_names keyword of banyan_sigma(). You can also call banyan_sigma() with the use_plx=True keyword to read them, or with use_plx=False to avoid this warning message.\n",
      "  #Main BANYAN_SIGMA routine\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.str_' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/GitHub/banyan_sigma/banyan_sigma.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/GitHub/banyan_sigma/banyan_sigma.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.str_' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Main BANYAN_SIGMA routine\n",
    "#Automatically detect Astropy Tables and transform them to pandas dataframes\n",
    "if stars_data is not None:\n",
    "    if isinstance(stars_data,Table):\n",
    "        #First remove multi-dimensional columns to avoid crash\n",
    "        for keys in stars_data.keys():\n",
    "            if stars_data[keys].ndim != 1:\n",
    "                stars_data.remove_column(keys)\n",
    "        #Now transform to pandas dataframe\n",
    "        stars_data = stars_data.to_pandas()\n",
    "\n",
    "#Check input consistency\n",
    "if stars_data is None and (ra is None or dec is None or pmra is None or pmdec is None or epmra is None or epmdec is None):\n",
    "    raise ValueError('Either an input structure (stars_data) or all of the ra,dec,pmra,pmdec,epmra and epmdec keywords must be specified !')\n",
    "\n",
    "if constraint_dist_per_hyp is not None and constraint_edist_per_hyp is None:\n",
    "    raise ValueError('f constraint_dist_per_hyp is specified, constraint_edist_per_hyp must also be specified !')\n",
    "\n",
    "#Default column names\n",
    "default_column_names = {'RA':'RA','DEC':'DEC','PMRA':'PMRA','PMDEC':'PMDEC','EPMRA':'EPMRA','EPMDEC':'EPMDEC'}\n",
    "if use_rv is True:\n",
    "    default_column_names['RV'] = 'RV'\n",
    "    default_column_names['ERV'] = 'ERV'\n",
    "if use_plx is True:\n",
    "    default_column_names['PLX'] = 'PLX'\n",
    "    default_column_names['EPLX'] = 'EPLX'\n",
    "if use_dist is True:\n",
    "    default_column_names['DIST'] = 'DIST'\n",
    "    default_column_names['EDIST'] = 'EDIST'\n",
    "if use_psi is True:\n",
    "    default_column_names['PSIRA'] = 'PSIRA'\n",
    "    default_column_names['PSIDEC'] = 'PSIDEC'\n",
    "    default_column_names['EPSIRA'] = 'EPSIRA'\n",
    "    default_column_names['EPSIDEC'] = 'EPSIDEC'\n",
    "\n",
    "#Merge user-issued column data with the default values (the user-issued values take predominance)\n",
    "if column_names is not None:\n",
    "    column_names = {**default_column_names, **column_names}\n",
    "else:\n",
    "    column_names = default_column_names\n",
    "\n",
    "#Check if a column named PLX, DIST, RV, PSIRA, etc. exist in stars_data but not in column_names. If this is the case, issue a warning so that the user understands that some data are not being considered.\n",
    "if stars_data is not None:\n",
    "    if 'PLX' in stars_data.keys() and 'PLX' not in column_names.keys() and use_plx is None:\n",
    "        warnings.warn('Parallaxes (PLX) were not read from the input data, because the PLX key was not included in the column_names keyword of banyan_sigma(). You can also call banyan_sigma() with the use_plx=True keyword to read them, or with use_plx=False to avoid this warning message.')\n",
    "    if 'DIST' in stars_data.keys() and 'DIST' not in column_names.keys() and use_dist is None:\n",
    "        warnings.warn('Distances (DIST) were not read from the input data, because the DIST key was not included in the column_names keyword of banyan_sigma(). You can also call banyan_sigma() with the use_dist=True keyword to read them, or with use_dist=False to avoid this warning message.')\n",
    "    if 'RV' in stars_data.keys() and 'RV' not in column_names.keys() and use_rv is None:\n",
    "        warnings.warn('Radial velocities (RV) were not read from the input data, because the RV key was not included in the column_names keyword of banyan_sigma(). You can also call banyan_sigma() with use_rv=True to read them, or with use_rv=False to avoid this warning message.')\n",
    "    if ('PSIRA' in stars_data.keys() and 'PSIRA' not in column_names.keys()) or ('PSIDEC' in stars_data.keys() and 'PSIDEC' not in column_names.keys()) and use_psi is None:\n",
    "        warnings.warn('The PSI parameters (PSIRA,PSIDEC) were not read from the input data, because the PSIRA and PSIDEC keys were not included in the column_data keyword of banyan_sigma(). You can also call banyan_sigma() with use_psi=True keyword to read them, or with use_psi=False to avoid this warning message.')\n",
    "\n",
    "#Create a table of data for BANYAN SIGMA to use\n",
    "if ra is not None:\n",
    "    nobj = np.size(ra)\n",
    "    zeros = np.zeros(nobj)\n",
    "    data_table = pd.DataFrame({'RA':ra,'DEC':dec,'PMRA':pmra,'PMDEC':pmdec,'EPMRA':epmra,'EPMDEC':epmdec,'PSIRA':zeros,'PSIDEC':zeros,'EPSIRA':zeros,'EPSIDEC':zeros})\n",
    "if ra is None:\n",
    "    nobj = stars_data.shape[0]\n",
    "    zeros = np.zeros(nobj)\n",
    "    data_table = pd.DataFrame({'RA':stars_data[column_names['RA']],'DEC':stars_data[column_names['DEC']],'PMRA':stars_data[column_names['PMRA']],'PMDEC':stars_data[column_names['PMDEC']],'EPMRA':stars_data[column_names['EPMRA']],'EPMDEC':stars_data[column_names['EPMDEC']],'PSIRA':zeros,'PSIDEC':zeros,'EPSIRA':zeros,'EPSIDEC':zeros})\n",
    "\n",
    "#Fill up the data table with stars_data if it is specified\n",
    "if stars_data is not None:\n",
    "    for keys in column_names.keys():\n",
    "        #Skip special keys\n",
    "        if (keys == 'NAME') or (keys == 'PLX') or (keys == 'EPLX'):\n",
    "            continue\n",
    "        data_table[keys] = stars_data[column_names[keys]]\n",
    "    if 'PLX' in column_names.keys():\n",
    "        data_table['DIST'] = 1e3/stars_data[column_names['PLX']]\n",
    "    if 'PLX' in column_names.keys() and 'EPLX' in column_names.keys():\n",
    "        data_table['EDIST'] = 1e3/stars_data[column_names['PLX']]**2*stars_data[column_names['EPLX']]\n",
    "\n",
    "#Transform parallaxes to distances directly in data_table\n",
    "if 'PLX' in data_table.keys() and 'EPLX' in data_table.keys():\n",
    "    data_table['EDIST'] = 1e3/data_table['PLX']**2*data_table['EPLX']\n",
    "    data_table = data_table.drop('EPLX', 1)\n",
    "if 'PLX' in data_table.keys():\n",
    "    data_table['DIST'] = 1e3/data_table['PLX']\n",
    "    data_table = data_table.drop('PLX', 1)\n",
    "\n",
    "#If measurements are specified as keywords, put them in the data table\n",
    "if ra is not None:\n",
    "    data_table['RA'] = ra\n",
    "if dec is not None:\n",
    "    data_table['DEC'] = dec\n",
    "if pmra is not None:\n",
    "    data_table['PMRA'] = pmra\n",
    "if pmdec is not None:\n",
    "    data_table['PMDEC'] = pmdec\n",
    "if epmra is not None:\n",
    "    data_table['EPMRA'] = epmra\n",
    "if epmdec is not None:\n",
    "    data_table['EPMDEC'] = epmdec\n",
    "if plx is not None:\n",
    "    data_table['DIST'] = 1e3/plx\n",
    "if plx is not None and eplx is not None:\n",
    "    data_table['EDIST'] = 1e3/plx**2*eplx\n",
    "if dist is not None:\n",
    "    data_table['DIST'] = dist\n",
    "if edist is not None:\n",
    "    data_table['EDIST'] = edist\n",
    "if rv is not None:\n",
    "    data_table['RV'] = rv\n",
    "if erv is not None:\n",
    "    data_table['ERV'] = erv\n",
    "if psira is not None:\n",
    "    data_table['PSIRA'] = psira\n",
    "if psidec is not None:\n",
    "    data_table['PSIDEC'] = psidec\n",
    "if epsira is not None:\n",
    "    data_table['EPSIRA'] = epsira\n",
    "if epsidec is not None:\n",
    "    data_table['EPSIDEC'] = epsidec\n",
    "\n",
    "#Check for unphysical data\n",
    "if np.max((data_table['RA'] < 0.) | (data_table['RA'] >= 360.)) != 0:\n",
    "    raise ValueError('Some RA values are unphysical')\n",
    "if np.max((data_table['DEC'] < -90.) | (data_table['DEC'] > 90.)) != 0:\n",
    "    raise ValueError('Some DEC values are unphysical')\n",
    "if np.max((data_table['EPMRA'] < 0.) | (data_table['EPMDEC'] < 0.)) != 0:\n",
    "    raise ValueError('Some EPMRA or EPMDEC values are unphysical')\n",
    "if np.max((np.isnan(data_table['RA']) | (np.isnan(data_table['DEC'])) | (np.isnan(data_table['PMRA'])) | (np.isnan(data_table['PMDEC'])) | (np.isnan(data_table['EPMRA'])) | (np.isnan(data_table['EPMDEC'])))) != 0:\n",
    "    raise ValueError('The observables ra,dec,pmra,pmdec,epmra and epmdec must be specified (and finite) for each object !')\n",
    "if 'RV' in data_table.keys() and 'ERV' not in data_table.keys():\n",
    "    raise ValueError('RV is defined in the data table but not ERV')\n",
    "if 'DIST' in data_table.keys() and 'EDIST' not in data_table.keys():\n",
    "    raise ValueError('DIST is defined in the data table but not EDIST')\n",
    "if 'ERV' in data_table.keys():\n",
    "    if np.max(data_table['ERV'] <= 0.):\n",
    "        raise ValueError('Some ERV values are unphysical')\n",
    "if 'RV' in data_table.keys() and 'ERV' in data_table.keys():\n",
    "    if np.max(np.isfinite(data_table['RV']) & np.isnan(data_table['ERV'])):\n",
    "        raise ValueError('Some RV values are specified without ERV')\n",
    "if 'DIST' in data_table.keys() and 'EDIST' in data_table.keys():\n",
    "    if np.max((data_table['DIST'] < 0.) | (data_table['EDIST'] <= 0.)):\n",
    "        raise ValueError('Some DIST or EDIST values are unphysical')\n",
    "    if np.max(np.isfinite(data_table['DIST']) & np.isnan(data_table['EDIST'])):\n",
    "        raise ValueError('Some DIST values are specified without EDIST')\n",
    "if np.max(((data_table['PSIRA'] != 0.) | (data_table['PSIDEC'] != 0.)) & ((data_table['EPSIRA'] == 0.) | (data_table['EPSIDEC'] == 0.)) | (data_table['EPSIRA'] < 0.) | (data_table['EPSIDEC'] < 0.)):\n",
    "        raise ValueError('Some EPSIRA or EPSIDEC values are unphysical')\n",
    "\n",
    "#Fill the data table with empty RVs and distances if they were not specified\n",
    "if 'RV' not in data_table.keys():\n",
    "    data_table['RV'] = np.nan\n",
    "if 'ERV' not in data_table.keys():\n",
    "    data_table['ERV'] = np.nan\n",
    "if 'DIST' not in data_table.keys():\n",
    "    data_table['DIST'] = np.nan\n",
    "if 'EDIST' not in data_table.keys():\n",
    "    data_table['EDIST'] = np.nan\n",
    "\n",
    "#Data file containing the parameters of Bayesian hypotheses\n",
    "parameters_file = os.path.dirname(__file__)+os.sep+'data'+os.sep+'banyan_sigma_parameters.fits'\n",
    "\n",
    "#Check if the file exists\n",
    "if not os.path.isfile(parameters_file):\n",
    "    raise ValueError('The multivariate Gaussian parameters file could not be found ! Please make sure that you did not move \"'+os.sep+'data'+os.sep+'banyan_sigma_parameters.fits\" from the same path as the Python file banyan_sigma.py !')\n",
    "\n",
    "#Read the parameters of Bayesian hypotheses\n",
    "parameters_str = Table.read(parameters_file,format='fits')\n",
    "#Remove white spaces in names\n",
    "parameters_str['NAME'] = np.chararray.strip(np.array(parameters_str['NAME']))\n",
    "#Index the table by hypothesis name\n",
    "parameters_str.add_index('NAME')\n",
    "npar = np.size(parameters_str)\n",
    "\n",
    "#Build a unique list of Bayesian hypotheses\n",
    "if hypotheses is None:\n",
    "    hypotheses = np.array(parameters_str['NAME'])\n",
    "    indexes = np.unique(hypotheses,return_index=True)[1]\n",
    "    hypotheses = hypotheses[sorted(indexes)]\n",
    "    hypotheses = np.array([hyp.decode(\"utf-8\") for hyp in hypotheses])\n",
    "    hypotheses = np.array([hyp.upper() for hyp in hypotheses.tolist()])\n",
    "\n",
    "#Make sure that hypotheses are all upper case\n",
    "#hypotheses = np.array([hyp.upper() for hyp in hypotheses.tolist()])\n",
    "#hypotheses = np.array([hyp.decode(\"utf-8\") for hyp in hypotheses])\n",
    "nhyp = hypotheses.size\n",
    "\n",
    "#If constraint_dist_per_hyp is set, check that all hypotheses are included\n",
    "if constraint_dist_per_hyp is not None:\n",
    "    if sorted(constraint_dist_per_hyp.keys()) != sorted(constraint_edist_per_hyp.keys()):\n",
    "        raise ValueError('The tag names of constraint_dist_per_hyp and constraint_edist_per_hyp are different')\n",
    "    if sorted(constraint_dist_per_hyp.keys()) != sorted(hypotheses.tolist()):\n",
    "        raise ValueError('The tag names of constraint_dist_per_hyp and the list of Bayesian hypotheses are different')\n",
    "\n",
    "    #Build constraint_dist_per_hyp into an array\n",
    "    dist_per_hyp_arr = np.empty((nobj,nhyp))*np.nan\n",
    "    edist_per_hyp_arr = np.empty((nobj,nhyp))*np.nan\n",
    "    #Read the distance constraints for each Bayesian hypothesis\n",
    "    for i in range(nhyp):\n",
    "        dist_per_hyp_arr[:,i] = constraint_dist_per_hyp[hypotheses[i]]\n",
    "        edist_per_hyp_arr[:,i] = constraint_edist_per_hyp[hypotheses[i]]\n",
    "\n",
    "    #Verify that all distance constraints are physical\n",
    "    if np.max(dist_per_hyp_arr < 0. | edist_per_hyp_arr <= 0.):\n",
    "        raise ValueError('Some of the specified constraint_dist_per_hyp or constraint_edist_per_hyp values are unphysical')\n",
    "    if np.max(np.isfinite(dist_per_hyp_arr) & np.isnan(edist_per_hyp_arr)):\n",
    "        raise ValueError('Some of the specified constraint_edist_per_hyp are not finite where constraint_dist_per_hyp are finite')\n",
    "\n",
    "    #Check that either all or none of the distance constraints are finite for a given object\n",
    "    if np.max(np.isfinite(np.nansum(dist_per_hyp_arr,axis=1)) and np.isnan(np.sum(dist_per_hyp_arr,axis=1))):\n",
    "        raise ValueError('The constraint_dist_per_hyp and constraint_edist_per_hyp values must be all finite or all non-finite for a given star')\n",
    "\n",
    "#Override priors to unity if the keyword unit_priors is set\n",
    "if unit_priors is True:\n",
    "    parameters_str['LN_PRIOR'] = 0.\n",
    "\n",
    "#Determine whether a trigonometric distance or a per-hypothesis distance constraint was set\n",
    "if constraint_dist_per_hyp is not None:\n",
    "    distance_is_set = (np.isfinite(data_table['DIST']) | np.isfinite(np.nansum(dist_per_hyp_arr,axis=1)))\n",
    "else:\n",
    "    distance_is_set = np.isfinite(data_table['DIST'])\n",
    "\n",
    "#Assign the correct Bayesian priors to each star\n",
    "g_pm = (np.where(np.isnan(data_table['RV']) & (~distance_is_set)))[0]\n",
    "g_pm_rv = (np.where(np.isfinite(data_table['RV']) & (~distance_is_set)))[0]\n",
    "g_pm_dist = (np.where(np.isnan(data_table['RV']) & distance_is_set))[0]\n",
    "g_pm_rv_dist = (np.where(np.isfinite(data_table['RV']) & distance_is_set))[0]\n",
    "ln_priors_nd = np.zeros((nobj,nhyp))\n",
    "ln_priors_nd_manual = np.zeros((nobj,nhyp))\n",
    "for i in range(nhyp):\n",
    "    #Skip the field hypotheses as they do not have a Bayesian prior\n",
    "    if hypotheses[i].find('FIELD') != -1:\n",
    "        continue\n",
    "    #Read the parameters structure to identify the 4 priors associated with a given young association\n",
    "    ln_priors_i = parameters_str.loc[hypotheses[i]]['LN_PRIOR']\n",
    "    #In the cases where only one prior is designated, assign it to all stars\n",
    "    if ln_priors_i.size == 1:\n",
    "        ln_priors_nd[:,i] = ln_priors_i[0]\n",
    "    else:\n",
    "        #Otherwise assign them properly as a function of available observables\n",
    "        ln_priors_nd[g_pm,i] = ln_priors_i[0]\n",
    "        ln_priors_nd[g_pm_rv,i] = ln_priors_i[1]\n",
    "        ln_priors_nd[g_pm_dist,i] = ln_priors_i[2]\n",
    "        ln_priors_nd[g_pm_rv_dist,i] = ln_priors_i[3]\n",
    "\n",
    "#Include manual priors if they are specified as an input structure\n",
    "if ln_priors is not None:\n",
    "    for i in range(nhyp):\n",
    "        #The field hypotheses *can* have manual priors\n",
    "        if hypotheses[i] not in ln_priors.keys():\n",
    "            warnings.warn('The prior for hypothesis '+hypotheses[i]+' was left to its default value as it was not specified manually')\n",
    "            continue\n",
    "        ln_priors_nd_manual[:,i] = ln_priors[hypotheses[i]]\n",
    "\n",
    "    #Normalize manual priors with the field hypothesis (because they get applied only on young associations)\n",
    "    gnorm = np.where(['FIELD' in hyp for hyp in hypotheses.tolist()])\n",
    "    norm_priors_1d = logsumexp(ln_priors_nd_manual[:,gnorm[0]],axis=1)\n",
    "    ln_priors_nd_manual -= np.tile(norm_priors_1d,(nhyp,1)).transpose()\n",
    "\n",
    "    #Apply the manual priors on top of the default priors\n",
    "    ln_priors_nd += ln_priors_nd_manual\n",
    "\n",
    "#If both trigonometric distances and per-hypothesis distance constraints are set, transform the per-hypothesis distance constraints into priors\n",
    "both_distances_set = []\n",
    "if constraint_dist_per_hyp is not None:\n",
    "    both_distances_set = np.where(np.isfinite(data_table['DIST']) & np.isfinite(np.nansum(dist_per_hyp_arr,axis=1)))\n",
    "if np.size(both_distances_set) != 0:\n",
    "    xdist_measured = np.tile(data_table['DIST'].iloc[both_distances_set[0]],(nhyp,1)).transpose()\n",
    "    xedist_measured = np.tile(data_table['EDIST'].iloc[both_distances_set[0]],(nhyp,1)).transpose()\n",
    "    ln_prob_dist_differences = -(xdist_measured-dist_per_hyp_arr[both_distances_set[0],:])**2/(2.0*(xedist_measured**2+edist_per_hyp_arr[both_distances_set[0],:]**2))\n",
    "\n",
    "    #Treat these values as priors so normalize them with the field hypotheses (because they get applied only on young associations)\n",
    "    gnorm = np.where(['FIELD' in hyp for hyp in hypotheses.tolist()])\n",
    "    norm_priors_1d = logsumexp(ln_prob_dist_differences[:,gnorm[0]],axis=1)\n",
    "    ln_prob_dist_differences -= np.tile(norm_priors_1d,(nhyp,1)).transpose()\n",
    "\n",
    "    #Apply these values on the priors\n",
    "    ln_priors_nd[both_distances_set[0],L] += ln_prob_dist_differences\n",
    "\n",
    "    #Remove the per-hypothesis distance constraints on these particular objects and just keep the trigonometric distances\n",
    "    dist_per_hyp_arr[both_distances_set[0],:] = np.nan\n",
    "    edist_per_hyp_arr[both_distances_set[0],:] = np.nan\n",
    "\n",
    "#Initiate an array that will contain the ln probabilities if those are the only required outputs\n",
    "if lnp_only is True:\n",
    "    all_lnprobs = np.empty((nobj,nhyp))*np.nan\n",
    "\n",
    "#Loop on hypotheses to run BANYAN Sigma on\n",
    "output_str_allhyps_list = []\n",
    "for i in range(nhyp):\n",
    "    #print(\"HYP \"+str(i))\n",
    "\n",
    "    #If constraint_dist_per_hyp is set, determine which distance constraint must be used now\n",
    "    dist_for_this_hypothesis = data_table['DIST'].values\n",
    "    edist_for_this_hypothesis = data_table['EDIST'].values\n",
    "    if constraint_dist_per_hyp is not None:\n",
    "        gdist_per_hyp = np.where(np.isfinite(dist_per_hyp_arr[:,i]))\n",
    "        dist_for_this_hypotheses[gdist_per_hyp[0]] = dist_per_hyp[gdist_per_hyp[0],i]\n",
    "        edist_for_this_hypotheses[gdist_per_hyp[0]] = edist_per_hyp_arr[gdist_per_hyp[0],i]\n",
    "\n",
    "    #Loop over individual multivariate Gaussians if the model is a mixture\n",
    "    ngauss = np.size(parameters_str.loc[hypotheses[i]])\n",
    "\n",
    "    output_str_multimodel_list = []\n",
    "    if lnp_only is True:\n",
    "        all_lnprobs_hypi = np.zeros((nobj,ngauss))\n",
    "\n",
    "    for gaussi in range(ngauss):\n",
    "\n",
    "        #Somehow we cannot access the Gaussian index without the table breaking when there is just one Gaussian component, so here we grab the right table row\n",
    "        if ngauss == 1:\n",
    "            parameters_str_row = parameters_str.loc[hypotheses[i]]\n",
    "        else:\n",
    "            parameters_str_row = parameters_str.loc[hypotheses[i]][gaussi]\n",
    "\n",
    "        #Determine how many batches will be needed to avoid saturating the RAM\n",
    "        nbatches = np.int(np.ceil(nobj/ntargets_max))\n",
    "        output_str_list = []\n",
    "        for ci in range(nbatches):\n",
    "            #Determine the indices of the stars to be selected\n",
    "            ind_from = np.int(np.round(ci*ntargets_max))\n",
    "            ind_to = np.int(ind_from + np.round(ntargets_max))\n",
    "            ind_to = np.minimum(ind_to,np.int(nobj))\n",
    "\n",
    "            #Create a sub-structure of input data\n",
    "            data_table_ci = data_table[ind_from:ind_to]\n",
    "            dist_for_this_hypothesis_ci = dist_for_this_hypothesis[ind_from:ind_to]\n",
    "            edist_for_this_hypothesis_ci = edist_for_this_hypothesis[ind_from:ind_to]\n",
    "            nobj_ci = np.size(data_table_ci)\n",
    "\n",
    "            #Solve the BANYAN Sigma integrals for this hypothesis and this batch of targets\n",
    "            output_str_ci = banyan_sigma_solve_multivar(data_table_ci['RA'].values,data_table_ci['DEC'].values,data_table_ci['PMRA'].values,data_table_ci['PMDEC'].values,data_table_ci['EPMRA'].values,data_table_ci['EPMDEC'].values,rv_measured=data_table_ci['RV'].values,rv_error=data_table_ci['ERV'].values,dist_measured=dist_for_this_hypothesis_ci,dist_error=edist_for_this_hypothesis_ci,psira=data_table_ci['PSIRA'].values,psidec=data_table_ci['PSIDEC'].values,psira_error=data_table_ci['EPSIRA'].values,psidec_error=data_table_ci['EPSIDEC'].values,precision_matrix=parameters_str_row['PRECISION_MATRIX'],center_vec=parameters_str_row['CENTER_VEC'],precision_matrix_determinant=parameters_str_row['PRECISION_DETERM'])\n",
    "\n",
    "            #Store the log of probabilities if those are the only required output\n",
    "            if lnp_only is True:\n",
    "                all_lnprobs_hypi[ind_from:ind_to,gaussi] = output_str_ci['LN_P']\n",
    "                continue\n",
    "\n",
    "            #Append the dataframe in the Python list\n",
    "            output_str_list.append(output_str_ci)\n",
    "\n",
    "        #Contatenate the list of Dataframes\n",
    "        output_str = pd.concat(output_str_list,ignore_index=True)\n",
    "\n",
    "        #Reformat the output structure if this hypothesis is a multivariate Gaussian mixture\n",
    "        if ngauss != 1:\n",
    "            #Use column multi-indexing to add a second title to the columns, which corresponds to the ID if the Gaussian mixture component\n",
    "            dataframe_column_names = output_str.columns\n",
    "            output_str.columns = [np.array(dataframe_column_names),np.array(np.tile('Gauss'+str(gaussi),dataframe_column_names.size))]\n",
    "            output_str_multimodel_list.append(output_str)\n",
    "\n",
    "    #If only log probs are required, compile them in the main array\n",
    "    if lnp_only is True:\n",
    "        if ngauss == 1:\n",
    "            all_lnprobs[:,i] = all_lnprobs_hypi\n",
    "        else:\n",
    "            weights = parameters_str.loc[hypotheses[i]]['COEFFICIENT']\n",
    "            weights /= np.sum(weights)\n",
    "            all_lnprobs[:,i] = logsumexp(np.tile(np.log(weights),(nobj,1))+all_lnprobs_hypi,axis=1)\n",
    "        continue\n",
    "\n",
    "    #Reformat the output structure if there is more than one multivariate gaussian\n",
    "    if ngauss != 1:\n",
    "        #Concatenate the list of pandas dataframes into a single dataframe\n",
    "        output_str_multimodel = pd.concat(output_str_multimodel_list,axis=1)\n",
    "\n",
    "        #Create a 2D array of weights to combine the Gaussian mixture components\n",
    "        weights = parameters_str.loc[hypotheses[i]]['COEFFICIENT']\n",
    "        weights /= np.sum(weights)\n",
    "        logweights_2d = np.tile(np.log(weights),(nobj,1))\n",
    "\n",
    "        #Combine each column of the dataframe with a weighted average\n",
    "        output_str = pd.DataFrame()\n",
    "        for coli in output_str_multimodel.columns.get_level_values(0):\n",
    "            output_str[coli] = logsumexp(logweights_2d+output_str_multimodel[coli],axis=1)\n",
    "\n",
    "    #Use column multi-indexing to add a second title to the columns, which corresponds to the name of the Bayesian hypothesis\n",
    "    dataframe_column_names = output_str.columns\n",
    "    output_str.columns = [np.array(dataframe_column_names),np.array(np.tile(hypotheses[i],dataframe_column_names.size))]\n",
    "\n",
    "    #Add the dataframe to the per-hypothesis list of dataframes\n",
    "    output_str_allhyps_list.append(output_str)\n",
    "\n",
    "#Concatenate the list of pandas dataframes into a single dataframe\n",
    "output_str_all = pd.concat(output_str_allhyps_list,axis=1)\n",
    "\n",
    "#Fetch all log probabilities (if lnp_only is set, this variable already exists)\n",
    "if lnp_only is False:\n",
    "    all_lnprobs = output_str_all['LN_P'].values\n",
    "\n",
    "#Normalize probabilities directly in log space\n",
    "ln_norm_output = all_lnprobs - np.tile(logsumexp(all_lnprobs,axis=1),(nhyp,1)).transpose()\n",
    "\n",
    "#Compute [0,1] probabilities\n",
    "norm_output = np.exp(ln_norm_output)\n",
    "\n",
    "#Identify hypotheses that correspond to moving groups or associations\n",
    "yind = (np.where(np.array([hypothesis.find('FIELD') == -1 for hypothesis in hypotheses])))[0]\n",
    "\n",
    "#Create an array of normalized YMG probabilities (no field)\n",
    "ln_norm_output_only_ymg = all_lnprobs[:,yind] - np.tile(logsumexp(all_lnprobs[:,yind],axis=1),(yind.size,1)).transpose()\n",
    "\n",
    "#Calculate the weighted YMG prior\n",
    "ln_prior_moving_groups = logsumexp(ln_priors_nd[:,yind]+ln_norm_output_only_ymg,axis=1)\n",
    "\n",
    "#Identify hypotheses that correspond to the field\n",
    "ffind = (np.where(np.array([hypothesis.find('FIELD') != -1 for hypothesis in hypotheses])))[0]\n",
    "\n",
    "#Weight the priors w/r/t the Bayesian probabilities and project these priors onto the field. This is a way to avoid having the priors change the relative moving group probabilities, as their goal is strictly to maximize young association vs FIELD classification performance\n",
    "#Normalize probabilities directly in log space, projecting the inverse young association prior on the field probability\n",
    "ln_P_with_prior = all_lnprobs\n",
    "ln_P_with_prior[:,ffind] -= np.tile(ln_prior_moving_groups,(ffind.size,1)).transpose()\n",
    "#Renormalize\n",
    "ln_norm_output_prior = ln_P_with_prior - np.tile(logsumexp(ln_P_with_prior,axis=1),(nhyp,1)).transpose()\n",
    "\n",
    "#Return log probabilities if this is the only required output\n",
    "if lnp_only is True:\n",
    "    #return ln_norm_output_prior\n",
    "    print('return ln_norm_output_prior')\n",
    "\n",
    "#Compute [0,1] probabilities\n",
    "norm_output_prior = np.exp(ln_norm_output_prior)\n",
    "\n",
    "#Data file containing the parameters of Bayesian hypotheses\n",
    "metrics_computed = False\n",
    "metrics_file = os.path.dirname(__file__)+os.sep+'data'+os.sep+'banyan_sigma_metrics.fits'\n",
    "print(__file__)\n",
    "\n",
    "#Check if the file exists\n",
    "if not os.path.isfile(metrics_file):\n",
    "    warnings.warn('The performance metrics file could not be found ! Performance metrics will not be calculated. Please make sure that you did not move \"'+os.sep+'data'+os.sep+'banyan_sigma_metrics.fits\" from the same path as the Python file banyan_sigma.py !')\n",
    "\n",
    "#Avoid computing biased metrics if the unit_priors keyword was set\n",
    "if os.path.isfile(metrics_file) and unit_priors is False:\n",
    "    metrics_str = Table.read(metrics_file,format='fits')\n",
    "    #Remove white spaces in association names\n",
    "    metrics_str['NAME'] = np.chararray.strip(np.array(metrics_str['NAME']))\n",
    "    #Index the table by hypothesis name\n",
    "    metrics_str.add_index('NAME')\n",
    "\n",
    "    #Loop on young associations to determine their individual metrics\n",
    "    tpr = np.empty((nobj,yind.size))*np.nan\n",
    "    fpr = np.empty((nobj,yind.size))*np.nan\n",
    "    ppv = np.empty((nobj,yind.size))*np.nan\n",
    "    for yindi in range(yind.size):\n",
    "        #Calculate the individual normalized probabilities for a given young association\n",
    "        probs_yindi = np.exp(ln_norm_output_prior[:,yindi] - logsumexp(ln_norm_output_prior[:,[yindi,ffind[0]]],axis=1))\n",
    "        #Store the interpolated values depending on observables\n",
    "        if g_pm.size != 0:\n",
    "            mode_index = 0\n",
    "            tpr[g_pm,yindi] = np.interp(probs_yindi[g_pm],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['TPR'][mode_index,:])\n",
    "            fpr[g_pm,yindi] = np.interp(probs_yindi[g_pm],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['FPR'][mode_index,:])\n",
    "            ppv[g_pm,yindi] = np.interp(probs_yindi[g_pm],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['PPV'][mode_index,:])\n",
    "        if g_pm_rv.size != 0:\n",
    "            mode_index = 1\n",
    "            tpr[g_pm_rv,yindi] = np.interp(probs_yindi[g_pm_rv],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['TPR'][mode_index,:])\n",
    "            fpr[g_pm_rv,yindi] = np.interp(probs_yindi[g_pm_rv],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['FPR'][mode_index,:])\n",
    "            ppv[g_pm_rv,yindi] = np.interp(probs_yindi[g_pm_rv],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['PPV'][mode_index,:])\n",
    "        if g_pm_dist.size != 0:\n",
    "            mode_index = 2\n",
    "            tpr[g_pm_dist,yindi] = np.interp(probs_yindi[g_pm_dist],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['TPR'][mode_index,:])\n",
    "            fpr[g_pm_dist,yindi] = np.interp(probs_yindi[g_pm_dist],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['FPR'][mode_index,:])\n",
    "            ppv[g_pm_dist,yindi] = np.interp(probs_yindi[g_pm_dist],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['PPV'][mode_index,:])\n",
    "        if g_pm_rv_dist.size != 0:\n",
    "            mode_index = 3\n",
    "            tpr[g_pm_rv_dist,yindi] = np.interp(probs_yindi[g_pm_rv_dist],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['TPR'][mode_index,:])\n",
    "            fpr[g_pm_rv_dist,yindi] = np.interp(probs_yindi[g_pm_rv_dist],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['FPR'][mode_index,:])\n",
    "            ppv[g_pm_rv_dist,yindi] = np.interp(probs_yindi[g_pm_rv_dist],metrics_str.loc[hypotheses[yind[yindi]]]['PROBS'],metrics_str.loc[hypotheses[yind[yindi]]]['PPV'][mode_index,:])\n",
    "\n",
    "    #Build the combination weights\n",
    "    ln_weights = np.copy(ln_norm_output_only_ymg)\n",
    "    #Any group with less than 1% probability is ignored to avoid propagating potential NaNs\n",
    "    ln_weights[np.where(ln_weights < np.log(1e-2))] = np.log(tiny_number)\n",
    "    #Re-normalize weights\n",
    "    ln_weights -= np.tile(logsumexp(ln_weights,axis=1),(yind.size,1)).transpose()\n",
    "\n",
    "    #Calculate the weighted metrics\n",
    "    tpr_weighted = np.exp(logsumexp(np.log(np.maximum(tpr,tiny_number))+ln_weights,axis=1))\n",
    "    fpr_weighted = np.exp(logsumexp(np.log(np.maximum(fpr,tiny_number))+ln_weights,axis=1))\n",
    "    ppv_weighted = np.exp(logsumexp(np.log(np.maximum(ppv,tiny_number))+ln_weights,axis=1))\n",
    "    metrics_computed = True\n",
    "\n",
    "#Determine the most probable hypothesis\n",
    "most_probable_index = np.nanargmax(norm_output_prior,axis=1)\n",
    "\n",
    "#Loop on objects to determine lists of good hypotheses\n",
    "hyp_lists = []\n",
    "best_ya = []\n",
    "norm_output_only_ymg = np.exp(ln_norm_output_only_ymg)\n",
    "for obji in range(nobj):\n",
    "    #Identify all young associations with relative P>5%\n",
    "    ind_obji = (np.where(norm_output_only_ymg[obji,:] > .05))[0]\n",
    "    if len(ind_obji) == 0:\n",
    "        hyp_lists.append('FIELD')\n",
    "        best_ya.append('FIELD')\n",
    "        continue\n",
    "\n",
    "    #Find the most probable moving group\n",
    "    best_ya_ind = np.nanargmax(norm_output_only_ymg[obji,:])\n",
    "    best_ya.append(hypotheses[yind][best_ya_ind])\n",
    "\n",
    "    #Sort by decreasing P\n",
    "    ind_obji = ind_obji[np.flip(np.argsort(norm_output_only_ymg[obji,ind_obji]),axis=0)]\n",
    "    #Build a list of associations\n",
    "    if len(ind_obji) > 1:\n",
    "        hyp_lists.append(';'.join([x+y for x,y in zip(hypotheses[yind][ind_obji].tolist(),['('+str(x)+')' for x in np.round(norm_output_only_ymg[obji,ind_obji]*1e2).astype(int).tolist()])]))\n",
    "    if len(ind_obji) == 1:\n",
    "        hyp_lists.append(hypotheses[yind][best_ya_ind])\n",
    "\n",
    "#Build a final output dataframe\n",
    "output_final = pd.DataFrame()\n",
    "\n",
    "#Store the star names if they are given\n",
    "if 'NAME' in data_table.keys():\n",
    "    output_final['NAME'] = data_table['NAME']\n",
    "\n",
    "#Store global results\n",
    "output_final['YA_PROB'] = np.nansum(norm_output_prior[:,yind],axis=1)\n",
    "output_final['LIST_PROB_YAS'] = hyp_lists\n",
    "output_final['BEST_HYP'] = hypotheses[most_probable_index]\n",
    "output_final['BEST_YA'] = best_ya\n",
    "\n",
    "#Add a second column title \"General\"\n",
    "dataframe_column_names = output_final.columns\n",
    "output_final.columns = [np.array(dataframe_column_names),np.array(np.tile('Global',dataframe_column_names.size))]\n",
    "\n",
    "if metrics_computed is True:\n",
    "    output_final['TPR','Metrics'] = tpr_weighted\n",
    "    output_final['FPR','Metrics'] = fpr_weighted\n",
    "    output_final['PPV','Metrics'] = ppv_weighted\n",
    "    output_final['NFP','Metrics'] = fpr_weighted*total_besancon_objects\n",
    "\n",
    "#Create a Dataframe with all probabilities\n",
    "probs_frame = pd.DataFrame(norm_output_prior,columns=[np.array(np.tile('ALL',nhyp)),hypotheses])\n",
    "\n",
    "#Add the per-group stuff\n",
    "if metrics_computed is True:\n",
    "    output_final = pd.concat([output_str_all.swaplevel(axis=1),probs_frame,output_final.swaplevel(axis=1)[['Metrics']],output_final.swaplevel(axis=1)[['Global']].swaplevel(axis=1)],axis=1)\n",
    "else:\n",
    "    output_final = pd.concat([output_str_all.swaplevel(axis=1),probs_frame,output_final.swaplevel(axis=1)[['Global']].swaplevel(axis=1)],axis=1)\n",
    "\n",
    "#Add star names if they were provided\n",
    "if 'NAME' in data_table.keys():\n",
    "    output_final.index = data_table['NAME']\n",
    "\n",
    "##Return the final structure\n",
    "#return output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_final.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all = output['ALL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/obsidian/GitHub/banyan_sigma/banyan_sigma.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  #Import the necessary packages\n",
      "/Users/obsidian/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:2523: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.loc._setitem_with_indexer(key, value)\n",
      "/Users/obsidian/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:2511: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return self._setitem_slice(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "probs = output_all.values*1e2\n",
    "max_prob = 99.9\n",
    "probs = np.minimum(probs,max_prob)\n",
    "probs_formatted = np.round(np.minimum(probs,max_prob),1)\n",
    "output_all[:] = probs_formatted\n",
    "#print(output_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>118TAU</th>\n",
       "      <th>ABDMG</th>\n",
       "      <th>BPMG</th>\n",
       "      <th>CAR</th>\n",
       "      <th>CARN</th>\n",
       "      <th>CBER</th>\n",
       "      <th>COL</th>\n",
       "      <th>CRA</th>\n",
       "      <th>EPSC</th>\n",
       "      <th>ETAC</th>\n",
       "      <th>...</th>\n",
       "      <th>TAU</th>\n",
       "      <th>THA</th>\n",
       "      <th>THOR</th>\n",
       "      <th>TWA</th>\n",
       "      <th>UCL</th>\n",
       "      <th>UCRA</th>\n",
       "      <th>UMA</th>\n",
       "      <th>USCO</th>\n",
       "      <th>XFOR</th>\n",
       "      <th>FIELD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    118TAU  ABDMG  BPMG  CAR  CARN  CBER  COL  CRA  EPSC  ETAC  ...     TAU  \\\n",
       "0      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.0   \n",
       "1      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.4   \n",
       "2      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     1.0   \n",
       "3      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.9   \n",
       "4      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.0   \n",
       "5      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.0   \n",
       "6      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     1.7   \n",
       "7      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.1   \n",
       "8      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.3   \n",
       "9      0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.1   \n",
       "10     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.0   \n",
       "11     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.1   \n",
       "12     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.2   \n",
       "13     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.2   \n",
       "14     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.9   \n",
       "15     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...    72.2   \n",
       "16     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...    99.3   \n",
       "17     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...    60.9   \n",
       "18     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.4   \n",
       "19     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.3   \n",
       "20     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     1.0   \n",
       "21     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...    99.1   \n",
       "22     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     1.7   \n",
       "23     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     8.4   \n",
       "24     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.1   \n",
       "25     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.0   \n",
       "26     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.4   \n",
       "27     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.0   \n",
       "28     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.1   \n",
       "29     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.0   \n",
       "30     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.2   \n",
       "31     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...    14.0   \n",
       "32     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.0   \n",
       "33     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.3   \n",
       "34     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     6.3   \n",
       "35     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.2   \n",
       "36     0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  ...     0.2   \n",
       "\n",
       "    THA  THOR  TWA  UCL  UCRA  UMA  USCO  XFOR  FIELD  \n",
       "0   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "1   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.6  \n",
       "2   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.0  \n",
       "3   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.1  \n",
       "4   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "5   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "6   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   98.3  \n",
       "7   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "8   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.7  \n",
       "9   0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "10  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "11  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "12  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.8  \n",
       "13  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.8  \n",
       "14  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.1  \n",
       "15  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   27.8  \n",
       "16  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0    0.7  \n",
       "17  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   39.1  \n",
       "18  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.6  \n",
       "19  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.7  \n",
       "20  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.0  \n",
       "21  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0    0.9  \n",
       "22  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   98.3  \n",
       "23  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   91.6  \n",
       "24  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "25  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "26  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.6  \n",
       "27  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "28  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "29  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "30  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.8  \n",
       "31  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   86.0  \n",
       "32  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.9  \n",
       "33  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.7  \n",
       "34  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   93.7  \n",
       "35  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.8  \n",
       "36  0.0   0.0  0.0  0.0   0.0  0.0   0.0   0.0   99.8  \n",
       "\n",
       "[37 rows x 28 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all most probable RVs\n",
    "rv_opt = pd.DataFrame()\n",
    "erv_opt = pd.DataFrame()\n",
    "d_opt = pd.DataFrame()\n",
    "ed_opt = pd.DataFrame()\n",
    "for keys in output['ALL'].keys():\n",
    "    rv_opt[keys] = [np.round(output[keys]['RV_OPT'][0],1)]\n",
    "    erv_opt[keys] = [np.round(output[keys]['ERV_OPT'][0],1)]\n",
    "    d_opt[keys] = [np.round(output[keys]['D_OPT'][0],1)]\n",
    "    ed_opt[keys] = [np.round(output[keys]['ED_OPT'][0],1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0\n",
      "118TAU    8.3\n",
      "ABDMG   -30.7\n",
      "BPMG    -11.9\n",
      "CAR     -15.8\n",
      "CARN     -0.7\n",
      "CBER      6.9\n",
      "COL     -12.3\n",
      "CRA      40.8\n",
      "EPSC     -2.8\n",
      "ETAC     19.8\n",
      "HYA     -11.6\n",
      "IC2391  -16.2\n",
      "IC2602  -63.6\n",
      "LCC     -20.0\n",
      "OCT       2.2\n",
      "PL8     -30.3\n",
      "PLE      -8.9\n",
      "ROPH     16.5\n",
      "TAU      18.2\n",
      "THA      -8.2\n",
      "THOR    -13.2\n",
      "TWA      -7.3\n",
      "UCL     -13.0\n",
      "UCRA     -8.5\n",
      "UMA     -24.6\n",
      "USCO      0.7\n",
      "XFOR      1.7\n",
      "FIELD   121.9\n"
     ]
    }
   ],
   "source": [
    "print(rv_opt.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def banyan_sigma_solve_multivar(ra,dec,pmra,pmdec,pmra_error,pmdec_error,precision_matrix=None,center_vec=None,rv_measured=None,rv_error=None,dist_measured=None,dist_error=None,psira=None,psidec=None,psira_error=None,psidec_error=None,lnP_only=False,precision_matrix_determinant=None,debug=False):\n",
    "\t#PROBLEM: PSIRA_ERROR AND PSIDEC_ERROR ARE NOT USED ?\n",
    "\t\"\"\"\n",
    "\tSolve the radial velocity and distance marginalization integrals (if needed) and compute log(probability) with Bayes theorem for an array of stars and a single multivariate Gaussian XYZUVW model. This is a subroutine of banyan_sigma.\n",
    "\n",
    "\tTemporary note: multivar_model is IDL's \"association_structure\"\n",
    "\n",
    "\tparams (ra,dec): Sky position (degrees)\n",
    "\tparams (pmra,pmdec): Proper motion (mas/yr). pmra must include the cos(delta) term\n",
    "\tparams (pmra_error,pmdec_error): Measurement errors on proper motion (mas/yr)\n",
    "\tparam precision_matrix: Inverse of the covariance matrix [XYZUVW] of the multivariate Gaussian model (mixed units of pc and km/s)\n",
    "\tparam precision_matrix_determinant; [X]\n",
    "\tparam center_vec: Central XYZUVW position of the multivariate Gaussian model (mixed units of pc and km/s)\n",
    "\tparams (rv_measured,rv_error): Radial velocity measurement and error (km/s) - Optional inputs\n",
    "\tparams (dist_measured,dist_error): Distance measurement and error (pc) - Optional inputs\n",
    "\tparams (psira,psidec): Psi vector (described in Gagne et al., in preparation) describing the parallax motion of the star. This can be used to model the effect of parallax motion when a proper motion was measured from only two epochs ([mas/yr]) - Optional inputs\n",
    "\tparams (epsira,epsidec): Measurement errors of the psi vector ([mas/yr]) - Optional inputs\n",
    "\tkeyword full_statistical_errors: Compute [full statistical errors]\n",
    "\tkeyword lnP_only: Only return the ln(probability)\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Check for parameter consistency\n",
    "\tnum_stars = np.size(ra)\n",
    "\tif np.size(dec) != num_stars or np.size(pmra) != num_stars or np.size(pmdec) != num_stars or np.size(pmra_error) != num_stars or np.size(pmdec_error) != num_stars:\n",
    "\t\traise ValueError('The dimensions ra, dec, pmra, pmdec, pmra_error and pmdec_error do not agree. They must all be numpy arrays of the same length.')\n",
    "\n",
    "\t#Check for radial velocity keyword consistencies\n",
    "\tif rv_measured is not None or rv_error is not None:\n",
    "\t\tif np.size(rv_measured) != num_stars or np.size(rv_error) != num_stars:\n",
    "\t\t\traise ValueError('The dimensions of rv_measured or rv_error do not agree with those of ra, etc. They must all be numpy arrays of the same length.')\n",
    "\n",
    "\t#Check for distance keyword consistencies\n",
    "\tif dist_measured is not None or dist_error is not None:\n",
    "\t\tif np.size(dist_measured) != num_stars or np.size(dist_error) != num_stars:\n",
    "\t\t\traise ValueError('The dimensions of dist_measured or dist_error do not agree with those of ra, etc. They must all be numpy arrays of the same length.')\n",
    "\n",
    "\t#Check for psi keyword consistencies\n",
    "\tif psira is not None or psidec is not None or psira_error is not None or psidec_error is not None:\n",
    "\t\tif np.size(psira) != num_stars or np.size(psidec) != num_stars or np.size(psira_error) != num_stars or np.size(psidec_error) != num_stars:\n",
    "\t\t\traise ValueError('The dimensions of psira, psidec, psira_error or psidec_error do not agree with those of ra, etc. They must all be numpy arrays of the same length.')\n",
    "\n",
    "\t#Check that center_vec is a 6-elements array\n",
    "\tif np.shape(center_vec) != (6,):\n",
    "\t\traise ValueError('center_vec must be a 6-elements numpy array.')\n",
    "\n",
    "\t#Check that precision_matrix is a 6x6 matrix\n",
    "\tif np.shape(precision_matrix) != (6, 6):\n",
    "\t\traise ValueError('precision_matrix must be a 6x6-elements numpy array.')\n",
    "\n",
    "\t#Compute Galactic coordinates\n",
    "\t(gl,gb) = equatorial_galactic(ra,dec)\n",
    "\n",
    "\t#lambda is defined in Gagne et al. (2017, ApJS, X, Y, equation 7)\n",
    "\tcos_gl = np.cos(np.radians(gl))\n",
    "\tcos_gb = np.cos(np.radians(gb))\n",
    "\tsin_gl = np.sin(np.radians(gl))\n",
    "\tsin_gb = np.sin(np.radians(gb))\n",
    "\tlambda_vector = np.array([cos_gb*cos_gl,cos_gb*sin_gl,sin_gb]).transpose()\n",
    "\n",
    "\t#Build matrices A and B to convert sky quantities in the Galactic coordinates frame. The A matrix is defined in Gagne et al. (2017, ApJS, X, Y, equation 7)\n",
    "\tA_matrix = np.zeros((num_stars,3,3))\n",
    "\tcos_ra = np.cos(np.radians(ra))\n",
    "\tcos_dec = np.cos(np.radians(dec))\n",
    "\tsin_ra = np.sin(np.radians(ra))\n",
    "\tsin_dec = np.sin(np.radians(dec))\n",
    "\tA_matrix[:,0,0] = cos_ra * cos_dec\n",
    "\tA_matrix[:,1,0] = sin_ra * cos_dec\n",
    "\tA_matrix[:,2,0] = sin_dec\n",
    "\tA_matrix[:,0,1] = -sin_ra\n",
    "\tA_matrix[:,1,1] = cos_ra\n",
    "\tA_matrix[:,0,2] = -cos_ra * sin_dec\n",
    "\tA_matrix[:,1,2] = -sin_ra * sin_dec\n",
    "\tA_matrix[:,2,2] = cos_dec\n",
    "\n",
    "\t#The B matrix is not directly referenced in the BANYAN Sigma paper.\n",
    "\tB_matrix = matrix_set_product_A_single(TGAL,A_matrix)\n",
    "\n",
    "\t#The M vector is defined in Gagne et al. (2017, ApJS, X, Y, equation 7)\n",
    "\tM_vector = matrix_vector_set_product_v_single(B_matrix,np.array([1.0,0.0,0.0]))\n",
    "\n",
    "\t#The N vector is defined in Gagne et al. (2017, ApJS, X, Y, equation 7)\n",
    "\tN_vector_sub = np.array([np.zeros(num_stars), np.array(kappa*pmra), np.array(kappa*pmdec)]).transpose()\n",
    "\tN_vector = matrix_vector_set_product(B_matrix,N_vector_sub)\n",
    "\n",
    "\t#The varphi vector is defined in Gagne et al. (2017, ApJS, X, Y, equation 20)\n",
    "\tif psira is not None:\n",
    "\t\tvarphi_vector_sub = np.array([np.zeros(num_stars),np.array(kappa*psira), np.array(kappa*psidec)]).transpose()\n",
    "\t\tvarphi_vector = matrix_vector_set_product(B_matrix,varphi_vector_sub)\n",
    "\n",
    "\t#OMEGA is defined in Gagne et al. (2017, ApJS, X, Y, equation 6)\n",
    "\tzero_vector = np.zeros([num_stars,3])\n",
    "\tOMEGA_vector = np.concatenate((zero_vector,M_vector),axis=1)\n",
    "\n",
    "\t#GAMMA is defined in Gagne et al. (2017, ApJS, X, Y, equation 6)\n",
    "\tGAMMA_vector = np.concatenate((lambda_vector,N_vector),axis=1)\n",
    "\n",
    "\t#PHI is defined in Gagne et al. (2017, ApJS, X, Y, equation 20)\n",
    "\tif psira is not None:\n",
    "\t\tPHI_vector = np.concatenate((zero_vector,varphi_vector),axis=1)\n",
    "\n",
    "\t#tau is defined in Gagne et al. (2017, ApJS, X, Y, equation 5)\n",
    "\tTAU_vector = np.repeat(center_vec.reshape(1,6),num_stars,axis=0)\n",
    "\tif psira is not None:\n",
    "\t\tTAU_vector += PHI_vector\n",
    "\n",
    "\t#Take scalar products in multivariate space\n",
    "\tOMEGA_OMEGA = scalar_set_product_multivariate(OMEGA_vector,OMEGA_vector,precision_matrix)\n",
    "\tGAMMA_GAMMA = scalar_set_product_multivariate(GAMMA_vector,GAMMA_vector,precision_matrix)\n",
    "\tOMEGA_GAMMA = scalar_set_product_multivariate(OMEGA_vector,GAMMA_vector,precision_matrix)\n",
    "\tOMEGA_TAU = scalar_set_product_multivariate(OMEGA_vector,TAU_vector,precision_matrix)\n",
    "\tGAMMA_TAU = scalar_set_product_multivariate(GAMMA_vector,TAU_vector,precision_matrix)\n",
    "\tTAU_TAU = scalar_set_product_multivariate(TAU_vector,TAU_vector,precision_matrix)\n",
    "\n",
    "\t#If radial velocity or distance measurements are given, propagate them to the relevant scalar products\n",
    "\tif dist_measured is not None and dist_error is not None:\n",
    "\t\t#Find where measured distances are finite\n",
    "\t\tfinite_ind = np.where(np.isfinite(dist_measured) & np.isfinite(dist_error))\n",
    "\t\tif np.size(finite_ind) != 0:\n",
    "\t\t\tnorm = np.maximum(dist_error[finite_ind],1e-3)**2\n",
    "\t\t\tGAMMA_GAMMA[finite_ind] += 1.0/norm\n",
    "\t\t\tGAMMA_TAU[finite_ind] += dist_measured[finite_ind]/norm\n",
    "\t\t\tTAU_TAU[finite_ind] += dist_measured[finite_ind]**2/norm\n",
    "\tif rv_measured is not None and rv_error is not None:\n",
    "\t\t#Find where measured RVs are finite\n",
    "\t\tfinite_ind = np.where(np.isfinite(rv_measured) & np.isfinite(rv_error))\n",
    "\t\tif np.size(finite_ind) != 0:\n",
    "\t\t\tnorm = np.maximum(rv_error[finite_ind],1e-3)**2\n",
    "\t\t\tOMEGA_OMEGA[finite_ind] += 1.0/norm\n",
    "\t\t\tOMEGA_TAU[finite_ind] += rv_measured[finite_ind]/norm\n",
    "\t\t\tTAU_TAU[finite_ind] += rv_measured[finite_ind]**2/norm\n",
    "\n",
    "\t#Calculate the determinant of the precision matrix unless it is given as a parameter\n",
    "\tif precision_matrix_determinant is None:\n",
    "\t\tprecision_matrix_determinant = np.linalg.det(precision_matrix)\n",
    "\tif precision_matrix_determinant <= 0:\n",
    "\t\traise ValueError('The determinant of the precision matrix bust be positive and non-zero !')\n",
    "\n",
    "\t#Calculate optimal distance and radial velocity\n",
    "\tbeta = (GAMMA_GAMMA - OMEGA_GAMMA**2/OMEGA_OMEGA)/2.0\n",
    "\tif np.nanmin(beta) < 0:\n",
    "\t\traise ValueError('beta has an ill-defined value !')\n",
    "\tgamma = OMEGA_GAMMA*OMEGA_TAU/OMEGA_OMEGA - GAMMA_TAU\n",
    "\tdist_optimal = (np.sqrt(gamma**2+32.0*beta) - gamma) / (4.0*beta)\n",
    "\trv_optimal = (4.0 - GAMMA_GAMMA*dist_optimal**2 + GAMMA_TAU*dist_optimal)/(OMEGA_GAMMA*dist_optimal)\n",
    "\n",
    "\t#Create arrays that contain the measured RV and distance if available, or the optimal values otherwise\n",
    "\tdist_optimal_or_measured = dist_optimal\n",
    "\tif dist_measured is not None and dist_error is not None:\n",
    "\t\tfinite_ind = np.where(np.isfinite(dist_measured) & np.isfinite(dist_error))\n",
    "\t\tif np.size(finite_ind) != 0:\n",
    "\t\t\tdist_optimal_or_measured[finite_ind] = dist_measured[finite_ind]\n",
    "\trv_optimal_or_measured = rv_optimal\n",
    "\tif rv_measured is not None and rv_error is not None:\n",
    "\t\tfinite_ind = np.where(np.isfinite(rv_measured) & np.isfinite(rv_error))\n",
    "\t\tif np.size(finite_ind) != 0:\n",
    "\t\t\trv_optimal_or_measured[finite_ind] = rv_measured[finite_ind]\n",
    "\n",
    "\t#Propagate proper motion measurement errors\n",
    "\tEX = np.zeros(num_stars)\n",
    "\tEY = np.zeros(num_stars)\n",
    "\tEZ = np.zeros(num_stars)\n",
    "\t(U, V, W, EU, EV, EW) = equatorial_UVW(ra,dec,pmra,pmdec,rv_optimal_or_measured,dist_optimal_or_measured,pmra_error=pmra_error,pmdec_error=pmdec_error)\n",
    "\n",
    "\t#Determine by how much the diagonal of the covariance matrix must be inflated to account for the measurement errors\n",
    "\tcovariance_matrix = np.linalg.inv(precision_matrix)\n",
    "\tcovariance_diagonal = np.diag(covariance_matrix)\n",
    "\tinflation_array = np.array([EX,EY,EZ,EU,EV,EW]).transpose()\n",
    "\tinflation_factors = 1.0 + inflation_array**2/np.repeat(covariance_diagonal.reshape(1,6),num_stars,axis=0)\n",
    "\n",
    "\t#Calculate how much the determinant of the covariance matrices must be inflated\n",
    "\tinflation_covariance_determinant = np.exp(np.sum(np.log(inflation_factors),axis=1))\n",
    "\n",
    "\t#Make sure that no matrix becomes unphysical\n",
    "\tif np.nanmin(inflation_covariance_determinant) <= 0:\n",
    "\t\traise ValueError('At least one covariance matrix has a negative or null determinant as a consequence of the measurement errors !')\n",
    "\n",
    "\t#Calculate new determinants for the precision matrices\n",
    "\tprecision_matrix_inflated_determinant = precision_matrix_determinant/inflation_covariance_determinant\n",
    "\n",
    "\t#Apply this to the precision matrices\n",
    "\tprecision_matrix_inflated = matrix_set_inflation(precision_matrix, 1.0/np.sqrt(inflation_factors))\n",
    "\n",
    "\t#Recalculate the scalar products with new precision matrices\n",
    "\tOMEGA_OMEGA = scalar_set_product_multivariate_variablemetric(OMEGA_vector,OMEGA_vector,precision_matrix_inflated)\n",
    "\tGAMMA_GAMMA = scalar_set_product_multivariate_variablemetric(GAMMA_vector,GAMMA_vector,precision_matrix_inflated)\n",
    "\tOMEGA_GAMMA = scalar_set_product_multivariate_variablemetric(OMEGA_vector,GAMMA_vector,precision_matrix_inflated)\n",
    "\tOMEGA_TAU = scalar_set_product_multivariate_variablemetric(OMEGA_vector,TAU_vector,precision_matrix_inflated)\n",
    "\tGAMMA_TAU = scalar_set_product_multivariate_variablemetric(GAMMA_vector,TAU_vector,precision_matrix_inflated)\n",
    "\tTAU_TAU = scalar_set_product_multivariate_variablemetric(TAU_vector,TAU_vector,precision_matrix_inflated)\n",
    "\n",
    "\t#If radial velocity or distance measurements are given, propagate them to the relevant scalar products\n",
    "\tif dist_measured is not None and dist_error is not None:\n",
    "\t\t#Find where measured distances are finite\n",
    "\t\tfinite_ind = np.where(np.isfinite(dist_measured) & np.isfinite(dist_error))\n",
    "\t\tif np.size(finite_ind) != 0:\n",
    "\t\t\tnorm = np.maximum(dist_error[finite_ind],1e-3)**2\n",
    "\t\t\tGAMMA_GAMMA[finite_ind] += 1.0/norm\n",
    "\t\t\tGAMMA_TAU[finite_ind] += dist_measured[finite_ind]/norm\n",
    "\t\t\tTAU_TAU[finite_ind] += dist_measured[finite_ind]**2/norm\n",
    "\tif rv_measured is not None and rv_error is not None:\n",
    "\t\t#Find where measured RVs are finite\n",
    "\t\tfinite_ind = np.where(np.isfinite(rv_measured) & np.isfinite(rv_error))\n",
    "\t\tif np.size(finite_ind) != 0:\n",
    "\t\t\tnorm = np.maximum(rv_error[finite_ind],1e-3)**2\n",
    "\t\t\tOMEGA_OMEGA[finite_ind] += 1.0/norm\n",
    "\t\t\tOMEGA_TAU[finite_ind] += rv_measured[finite_ind]/norm\n",
    "\t\t\tTAU_TAU[finite_ind] += rv_measured[finite_ind]**2/norm\n",
    "\n",
    "\t#Update optimal distance and radial velocity\n",
    "\tbeta = (GAMMA_GAMMA - OMEGA_GAMMA**2/OMEGA_OMEGA)/2.0\n",
    "\tif np.nanmin(beta) < 0:\n",
    "\t\traise ValueError('beta has an ill-defined value !')\n",
    "\tgamma = OMEGA_GAMMA*OMEGA_TAU/OMEGA_OMEGA - GAMMA_TAU\n",
    "\tdist_optimal = (np.sqrt(gamma**2+32.0*beta) - gamma) / (4.0*beta)\n",
    "\trv_optimal = (4.0 - GAMMA_GAMMA*dist_optimal**2 + GAMMA_TAU*dist_optimal)/(OMEGA_GAMMA*dist_optimal)\n",
    "\n",
    "\t#Calculate error bars on the optimal distance and radial velocity\n",
    "\tedist_optimal = 1.0/np.sqrt(GAMMA_GAMMA)\n",
    "\terv_optimal = 1.0/np.sqrt(OMEGA_OMEGA)\n",
    "\n",
    "\t#Calculate final quantities for ln probability\n",
    "\tzeta = (TAU_TAU - OMEGA_TAU**2/OMEGA_OMEGA)/2.0\n",
    "\txarg = gamma/np.sqrt(2.0*beta)\n",
    "\n",
    "\tlnP_coeff = -0.5*np.log(OMEGA_OMEGA) - 2.5*np.log(beta) + 0.5*np.log(precision_matrix_inflated_determinant)\n",
    "\tlnP_part1 = xarg**2/2.0 - zeta\n",
    "\tlnP_part2 = np.log(np.maximum(parabolic_cylinder_f5_mod(xarg),tiny_number))\n",
    "\tlnP = lnP_coeff + lnP_part1 + lnP_part2\n",
    "\n",
    "\t#Return ln_P if only this is required\n",
    "\tif lnP_only:\n",
    "\t\treturn lnP\n",
    "\n",
    "\t#Create arrays that contain the measured RV and distance if available, or the optimal values otherwise\n",
    "\tdist_optimal_or_measured = dist_optimal\n",
    "\tedist_optimal_or_measured = edist_optimal\n",
    "\tif dist_measured is not None and dist_error is not None:\n",
    "\t\tfinite_ind = np.where(np.isfinite(dist_measured) & np.isfinite(dist_error))\n",
    "\t\tif np.size(finite_ind) != 0:\n",
    "\t\t\tdist_optimal_or_measured[finite_ind] = dist_measured[finite_ind]\n",
    "\t\t\tedist_optimal_or_measured[finite_ind] = dist_error[finite_ind]\n",
    "\trv_optimal_or_measured = rv_optimal\n",
    "\terv_optimal_or_measured = erv_optimal\n",
    "\tif rv_measured is not None and rv_error is not None:\n",
    "\t\tfinite_ind = np.where(np.isfinite(rv_measured) & np.isfinite(rv_error))\n",
    "\t\tif np.size(finite_ind) != 0:\n",
    "\t\t\trv_optimal_or_measured[finite_ind] = rv_measured[finite_ind]\n",
    "\t\t\terv_optimal_or_measured[finite_ind] = rv_error[finite_ind]\n",
    "\n",
    "\t#Calculate XYZ and UVW positions at the optimal (or measured) RV and distance\n",
    "\t(X, Y, Z, EX, EY, EZ) = equatorial_XYZ(ra,dec,dist_optimal_or_measured,dist_error=edist_optimal_or_measured)\n",
    "\t(U, V, W, EU, EV, EW) = equatorial_UVW(ra,dec,pmra,pmdec,rv_optimal_or_measured,dist_optimal_or_measured,pmra_error=pmra_error,pmdec_error=pmdec_error,rv_error=erv_optimal_or_measured,dist_error=edist_optimal_or_measured)\n",
    "\tXYZUVW = np.array([X,Y,Z,U,V,W]).transpose()\n",
    "\tEXYZUVW = np.array([EX,EY,EZ,EU,EV,EW]).transpose()\n",
    "\n",
    "\t#Calculate the Mahalanobis distance from the optimal position to the Gaussian model\n",
    "\tvec = XYZUVW - TAU_vector\n",
    "\tmahalanobis = np.sqrt(scalar_set_product_multivariate_variablemetric(vec,vec,precision_matrix_inflated))\n",
    "\n",
    "\t#Calculate the XYZ (pc) and UVW (km/s) separations from the optimal position to the center of the Gaussian model\n",
    "\tXYZ_sep = np.sqrt(np.sum((XYZUVW[:,0:3]-TAU_vector[:,0:3])**2,axis=1))\n",
    "\tUVW_sep = np.sqrt(np.sum((XYZUVW[:,3:6]-TAU_vector[:,3:6])**2,axis=1))\n",
    "\n",
    "\t#Calculate the 3D N-sigma distances from the optimal position to the center of the Gaussian models\n",
    "\tXYZ_sig = np.sqrt(scalar_set_product_multivariate_variablemetric(vec[:,0:3],vec[:,0:3],precision_matrix_inflated[:,0:3,0:3]))\n",
    "\tUVW_sig = np.sqrt(scalar_set_product_multivariate_variablemetric(vec[:,3:6],vec[:,3:6],precision_matrix_inflated[:,3:6,3:6]))\n",
    "\n",
    "\t#Store the data in a pandas dataframe\n",
    "\toutput_structure = pd.DataFrame(np.array([lnP,dist_optimal,rv_optimal,edist_optimal,erv_optimal,X,Y,Z,U,V,W,EX,EY,EZ,EU,EV,EW,XYZ_sep,UVW_sep,XYZ_sig,UVW_sig,mahalanobis]).transpose(),columns=['LN_P','D_OPT','RV_OPT','ED_OPT','ERV_OPT','X','Y','Z','U','V','W','EX','EY','EZ','EU','EV','EW','XYZ_SEP','UVW_SEP','XYZ_SIG','UVW_SIG','MAHALANOBIS'])\n",
    "\n",
    "\t#Return the output table\n",
    "\treturn output_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parabolic_cylinder_f5_mod(x):\n",
    "\t\"\"\"\n",
    "\tCalculates the real part of the \"modified\" Parabolic Cylinder Function D of index v=-5.\n",
    "\n",
    "\tThe regular function D(-5,x) is equivalent to the real part of:\n",
    "\t\tfrom scipy.special import pbdv\n",
    "\t\treturn pbdv(-5,x)\n",
    "\n",
    "\tAnd is equivalent to the mathematical expression:\n",
    "\t\texp(x^2/4)/24 * (sqrt(pi/2)*(x^4+6*x^2+3)*erfc(x/sqrt(2)) - exp(-x^2/2)*(x^3+5*x))\n",
    "\n",
    "\tThe modified parabolic cylinder does away with the exp(x^2/4) term to improve numerical stability, and instead returns:\n",
    "\t\t(sqrt(pi/2)*(x^4+6*x^2+3)*erfc(x/sqrt(2)) - exp(-x^2/2)*(x^3+5*x))/24\n",
    "\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Define shortcuts for efficiency\n",
    "\tsqrt2 = np.sqrt(2.)\n",
    "\tsqrt_halfpi = np.sqrt(np.pi)/sqrt2\n",
    "\tx_over_sqrt2 = x / sqrt2\n",
    "\terfc_x_over_sqrt2 = erfc(x_over_sqrt2)\n",
    "\tepsilon = np.exp(-x**2/2.0)\n",
    "\n",
    "\t#Calculate the output\n",
    "\ty = 1/24.0*(sqrt_halfpi*(x**4+6.*x**2+3.)*erfc_x_over_sqrt2 - epsilon*(x**3+5.*x))\n",
    "\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equatorial_galactic(ra,dec):\n",
    "\t\"\"\"Transforms equatorial coordinates (ra,dec) to Galactic coordinates (gl,gb). All inputs must be numpy arrays of the same dimension\n",
    "\n",
    "\t\tparam ra: Right ascension (degrees)\n",
    "\t\tparam dec: Declination (degrees)\n",
    "\t\toutput (gl,gb): Tuple containing Galactic longitude and latitude (degrees)\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Check for parameter consistency\n",
    "\tnum_stars = np.size(ra)\n",
    "\tif np.size(dec) != num_stars:\n",
    "\t\traise ValueError('The dimensions ra and dec do not agree. They must all be numpy arrays of the same length.')\n",
    "\n",
    "\n",
    "\t#ra_pol,dec_pol,l_north,sin_dec_pol,cos_dec_pol\n",
    "\tra_m_ra_pol = ra - ra_pol\n",
    "\tsin_ra = np.sin(np.radians(ra_m_ra_pol))\n",
    "\tcos_ra = np.cos(np.radians(ra_m_ra_pol))\n",
    "\tsin_dec = np.sin(np.radians(dec))\n",
    "\tcos_dec = np.cos(np.radians(dec))\n",
    "\n",
    "\t#Compute Galactic latitude\n",
    "\tgamma = sin_dec_pol*sin_dec + cos_dec_pol*cos_dec*cos_ra\n",
    "\tgb = np.degrees(np.arcsin(gamma))\n",
    "\n",
    "\t#Compute Galactic longitude\n",
    "\tx1 = cos_dec * sin_ra\n",
    "\tx2 = (sin_dec - sin_dec_pol*gamma)/cos_dec_pol\n",
    "\tgl = l_north - np.degrees(np.arctan2(x1,x2))\n",
    "\tgl = (gl+360.)%(360.)\n",
    "\n",
    "\t#Return galactic coordinates tuple\n",
    "\treturn (gl, gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_set_product_A_single(A,B):\n",
    "\t\"\"\"Performs matrix multiplication A#B where B is a set of N matrices. This function is more performant than looping over the N matrices if N is much larger than the matrix dimension D. A and the individual Bs must be square. The columns of A are multiplied by the rows of Bs. In IDL this function is called matrix_multiply_square_act.\n",
    "\n",
    "\tparam A: DxD numpy array\n",
    "\tparam B: NxDxD numpy array\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Verify matrix dimensions\n",
    "\tmatrix_dim = A.shape[0]\n",
    "\tset_size = B.shape[0]\n",
    "\tif A.shape[1] != matrix_dim or B.shape[1] != matrix_dim or B.shape[2] != matrix_dim:\n",
    "\t\traise ValueError('The dimensions D of matrices A and B do not agree - A must have dimension DxD and B must have dimension NxDxD')\n",
    "\tif np.size(A.shape) != 2 or np.size(B.shape) != 3:\n",
    "\t\traise ValueError('The number of dimensions of matrices A and B are not valid - A must have dimension DxD and B must have dimension NxDxD')\n",
    "\n",
    "\t#Initiate resulting matrix C and perform by-element matrix multiplication\n",
    "\tC = np.zeros([set_size,matrix_dim,matrix_dim])\n",
    "\tfor i in range(0,matrix_dim):\n",
    "\t\tfor j in range(0,matrix_dim):\n",
    "\t\t\tfor k in range(0,matrix_dim):\n",
    "\t\t\t\tC[:,i,j] += A[i,k] * B[:,k,j]\n",
    "\n",
    "\t#Return the resulting matrix\n",
    "\treturn C\n",
    "\n",
    "def matrix_vector_set_product_v_single(A,v):\n",
    "\t\"\"\"Performs matrix-vector multiplication A#v where A is a set of matrices and v is a single vector. This function is more performant than looping over the N sets if N is much larger than the matrix-vector dimension D. A must be square. Each column of A is multiplied by the vector v in a scalar product. In IDL this function is called matrix_vector_product_vct.\n",
    "\n",
    "\tparam A: NxDxD numpy array\n",
    "\tparam v: D numpy array\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Verify matrix dimensions\n",
    "\tmatrix_dim = A.shape[1]\n",
    "\tset_size = A.shape[0]\n",
    "\tif A.shape[2] != matrix_dim or v.shape[0] != matrix_dim:\n",
    "\t\traise ValueError('The dimensions D of matrix A and vector v do not agree - A must have dimension NxDxD and v must have dimension D')\n",
    "\tif np.size(A.shape) != 3 or np.size(v.shape) != 1:\n",
    "\t\traise ValueError('The number of dimensions of matrix A vector v are not valid - A must have dimension NxDxD and v must have dimension D')\n",
    "\n",
    "\t#Initiate resulting vector w and perform by-element matrix-vector multiplication\n",
    "\tw = np.zeros([set_size,matrix_dim])\n",
    "\tfor i in range(0,matrix_dim):\n",
    "\t\tfor k in range(0,matrix_dim):\n",
    "\t\t\tw[:,i] += A[:,i,k] * v[k]\n",
    "\n",
    "\t#Return the resulting vector\n",
    "\treturn w\n",
    "\n",
    "def matrix_vector_set_product(A,v):\n",
    "\t\"\"\"\n",
    "\tPerforms matrix-vector multiplication A#v where both A and v are sets of N matrices and N vectors. This function is more performant than looping over the N sets if N is much larger than the matrix-vector dimension D. A must be square. Each column of A is multiplied by the vector v in a scalar product. In IDL this function is called matrix_vector_product.\n",
    "\n",
    "\tparam A: NxDxD numpy array\n",
    "\tparam v: NxD numpy array\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Verify matrix dimensions\n",
    "\tmatrix_dim = A.shape[1]\n",
    "\tset_size = A.shape[0]\n",
    "\tif A.shape[2] != matrix_dim or v.shape[1] != matrix_dim:\n",
    "\t\traise ValueError('The dimensions D of matrix A and vector v do not agree - A must have dimension NxDxD and v must have dimension NxD')\n",
    "\tif np.size(A.shape) != 3 or np.size(v.shape) != 2:\n",
    "\t\traise ValueError('The number of dimensions of matrix A vector v are not valid - A must have dimension NxDxD and v must have dimension NxD')\n",
    "\n",
    "\t#Initiate resulting vector w and perform by-element matrix-vector multiplication\n",
    "\tw = np.zeros([set_size,matrix_dim])\n",
    "\tfor i in range(0,matrix_dim):\n",
    "\t\tfor k in range(0,matrix_dim):\n",
    "\t\t\tw[:,i] += A[:,i,k] * v[:,k]\n",
    "\n",
    "\t#Return the resulting vector\n",
    "\treturn w\n",
    "\n",
    "def scalar_set_product_multivariate(u,v,metric):\n",
    "\t\"\"\"\n",
    "\tPerforms scalar multiplication in a non-Euclidian metric u#(metric)#v. Both u and v are sets of N vectors. This function is more performant than looping over the N vectors if N is much larger than the vector dimension D. In IDL this function is called inner_product_multi.\n",
    "\n",
    "\tparam u: NxD numpy array\n",
    "\tparam v: NxD numpy array\n",
    "\tparam metric: DxD numpy array\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Verify matrix dimensions\n",
    "\tmatrix_dim = u.shape[1]\n",
    "\tset_size = u.shape[0]\n",
    "\tif v.shape[0] != set_size or v.shape[1] != matrix_dim:\n",
    "\t\traise ValueError('The dimensions of vectors u and v do not agree - both must have dimension NxD')\n",
    "\tif metric.shape[0] != matrix_dim or metric.shape[1] != matrix_dim:\n",
    "\t\traise ValueError('The dimensions of the metric are incompatible with vectors u and v - It must have dimension DxD where u and v have dimensions NxD')\n",
    "\tif np.size(u.shape) != 2 or np.size(v.shape) != 2 or np.size(metric.shape) != 2:\n",
    "\t\traise ValueError('The number of dimensions of vectors u, v and metric matrix are not valid - u and v must have dimension NxD and metric must have dimension DxD')\n",
    "\n",
    "\t#Initiate resulting scalar w and perform by-element matrix multiplication\n",
    "\tw = np.zeros(set_size)\n",
    "\tfor i in range(0,matrix_dim):\n",
    "\t\tfor j in range(0,matrix_dim):\n",
    "\t\t\tw += u[:,i] * v[:,j] * metric[i,j]\n",
    "\n",
    "\t#Return the resulting scalar\n",
    "\treturn w\n",
    "\n",
    "def scalar_set_product_multivariate_variablemetric(u,v,metric):\n",
    "\t\"\"\"\n",
    "\tPerforms scalar multiplication in a non-Euclidian metric u#(metric)#v. Both u and v are sets of N vectors, and \"metric\" is a set of matrices. This function is more performant than looping over the N vectors if N is much larger than the vector dimension D. In IDL this function is called inner_product_multi.\n",
    "\n",
    "\tparam u: NxD numpy array\n",
    "\tparam v: NxD numpy array\n",
    "\tparam metric: NxDxD numpy array\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Verify matrix dimensions\n",
    "\tmatrix_dim = u.shape[1]\n",
    "\tset_size = u.shape[0]\n",
    "\tif v.shape[0] != set_size or v.shape[1] != matrix_dim:\n",
    "\t\traise ValueError('The dimensions of vectors u and v do not agree - both must have dimension NxD')\n",
    "\tif metric.shape[0] != set_size or metric.shape[1] != matrix_dim or metric.shape[2] != matrix_dim:\n",
    "\t\traise ValueError('The dimensions of the metric are incompatible with vectors u and v - It must have dimension NxDxD where u and v have dimensions NxD')\n",
    "\tif np.size(u.shape) != 2 or np.size(v.shape) != 2 or np.size(metric.shape) != 3:\n",
    "\t\traise ValueError('The number of dimensions of vectors u, v and metric matrix are not valid - u and v must have dimension NxD and metric must have dimension NxDxD')\n",
    "\n",
    "\t#Initiate resulting scalar w and perform by-element matrix multiplication\n",
    "\tw = np.zeros(set_size)\n",
    "\tfor i in range(0,matrix_dim):\n",
    "\t\tfor j in range(0,matrix_dim):\n",
    "\t\t\tw += u[:,i] * v[:,j] * metric[:,i,j]\n",
    "\n",
    "\t#Return the resulting scalar\n",
    "\treturn w\n",
    "\n",
    "def matrix_set_inflation(A,v):\n",
    "\t\"\"\"\n",
    "\tPerforms the inflation of the diagonal of a single matrix A with set of factors v. This is the equivalent of v#A#v.\n",
    "\n",
    "\tparam A: DxD numpy array\n",
    "\tparam v: NxD numpy array\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Verify matrix dimensions\n",
    "\tmatrix_dim = A.shape[0]\n",
    "\tset_size = v.shape[0]\n",
    "\tif A.shape[1] != matrix_dim or v.shape[1] != matrix_dim:\n",
    "\t\traise ValueError('The dimensions of matrix A vector v do not agree - A must have dimension DxD and v must have dimension NxD')\n",
    "\tif np.size(A.shape) != 2 or np.size(v.shape) != 2:\n",
    "\t\traise ValueError('The number of dimensions of matrix A or vector v are not valid - A must have dimension DxD and v must have dimension NxD')\n",
    "\n",
    "\t#Calculate B = A#v\n",
    "\tB = np.empty((set_size,matrix_dim,matrix_dim))\n",
    "\tfor i in range(0,matrix_dim):\n",
    "\t\tfor j in range(0,matrix_dim):\n",
    "\t\t\tB[:,i,j] = A[i,j] * v[:,j]\n",
    "\n",
    "\t#Calculate C = v#B = v#A#v\n",
    "\tC = np.empty((set_size,matrix_dim,matrix_dim))\n",
    "\tfor i in range(0,matrix_dim):\n",
    "\t\tfor j in range(0,matrix_dim):\n",
    "\t\t\tC[:,i,j] = v[:,i] * B[:,i,j]\n",
    "\n",
    "\t#Return the resulting set of matrices\n",
    "\treturn C\n",
    "\n",
    "def equatorial_XYZ(ra,dec,dist,dist_error=None):\n",
    "\t\"\"\"\n",
    "\tTransforms equatorial coordinates (ra,dec) and distance to Galactic position XYZ. All inputs must be numpy arrays of the same dimension.\n",
    "\n",
    "\tparam ra: Right ascension (degrees)\n",
    "\tparam dec: Declination (degrees)\n",
    "\tparam dist: Distance (parsec)\n",
    "\tparam dist_error: Error on distance (parsec)\n",
    "\n",
    "\toutput (X,Y,Z): Tuple containing Galactic position XYZ (parsec)\n",
    "\toutput (X,Y,Z,EX,EY,EZ): Tuple containing Galactic position XYZ and their measurement errors, used if any measurement errors are given as inputs (parsec)\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Verify keywords\n",
    "\tnum_stars = np.size(ra)\n",
    "\tif np.size(dec) != num_stars or np.size(dist) != num_stars:\n",
    "\t\traise ValueError('ra, dec and distance must all be numpy arrays of the same size !')\n",
    "\tif dist_error is not None and np.size(dist_error) != num_stars:\n",
    "\t\traise ValueError('dist_error must be a numpy array of the same size as ra !')\n",
    "\n",
    "\t#Compute Galactic coordinates\n",
    "\t(gl, gb) = equatorial_galactic(ra,dec)\n",
    "\n",
    "\tcos_gl = np.cos(np.radians(gl))\n",
    "\tcos_gb = np.cos(np.radians(gb))\n",
    "\tsin_gl = np.sin(np.radians(gl))\n",
    "\tsin_gb = np.sin(np.radians(gb))\n",
    "\n",
    "\tX = cos_gb * cos_gl * dist\n",
    "\tY = cos_gb * sin_gl * dist\n",
    "\tZ = sin_gb * dist\n",
    "\n",
    "\tif dist_error is not None:\n",
    "\t\t#X_gb = sin_gb * cos_gl * dist * np.pi/180.\n",
    "\t\t#X_gl = cos_gb * sin_gl * dist * np.pi/180.\n",
    "\t\tX_dist = cos_gb * cos_gl\n",
    "\t\tEX = np.abs(X_dist * dist_error)\n",
    "\t\tY_dist = cos_gb * sin_gl\n",
    "\t\tEY = np.abs(Y_dist * dist_error)\n",
    "\t\tZ_dist = sin_gb\n",
    "\t\tEZ = np.abs(Z_dist * dist_error)\n",
    "\t\treturn (X, Y, Z, EX, EY, EZ)\n",
    "\telse:\n",
    "\t\treturn (X, Y, Z)\n",
    "\n",
    "def equatorial_UVW(ra,dec,pmra,pmdec,rv,dist,pmra_error=None,pmdec_error=None,rv_error=None,dist_error=None):\n",
    "\t\"\"\"\n",
    "\tTransforms equatorial coordinates (ra,dec), proper motion (pmra,pmdec), radial velocity and distance to space velocities UVW. All inputs must be numpy arrays of the same dimension.\n",
    "\n",
    "\tparam ra: Right ascension (degrees)\n",
    "\tparam dec: Declination (degrees)\n",
    "\tparam pmra: Proper motion in right ascension (milliarcsecond per year). \tMust include the cos(delta) term\n",
    "\tparam pmdec: Proper motion in declination (milliarcsecond per year)\n",
    "\tparam rv: Radial velocity (kilometers per second)\n",
    "\tparam dist: Distance (parsec)\n",
    "\tparam ra_error: Error on right ascension (degrees)\n",
    "\tparam dec_error: Error on declination (degrees)\n",
    "\tparam pmra_error: Error on proper motion in right ascension (milliarcsecond per year)\n",
    "\tparam pmdec_error: Error on proper motion in declination (milliarcsecond per year)\n",
    "\tparam rv_error: Error on radial velocity (kilometers per second)\n",
    "\tparam dist_error: Error on distance (parsec)\n",
    "\n",
    "\toutput (U,V,W): Tuple containing Space velocities UVW (kilometers per second)\n",
    "\toutput (U,V,W,EU,EV,EW): Tuple containing Space velocities UVW and their measurement errors, used if any measurement errors are given as inputs (kilometers per second)\n",
    "\t\"\"\"\n",
    "\n",
    "\t#Verify keywords\n",
    "\tnum_stars = np.size(ra)\n",
    "\tif np.size(dec) != num_stars or np.size(pmra) != num_stars or np.size(pmdec) != num_stars or np.size(dist) != num_stars:\n",
    "\t\traise ValueError('ra, dec, pmra, pmdec, rv and distance must all be numpy arrays of the same size !')\n",
    "\tif pmra_error is not None and np.size(pmra_error) != num_stars:\n",
    "\t\traise ValueError('pmra_error must be a numpy array of the same size as ra !')\n",
    "\tif pmdec_error is not None and np.size(pmdec_error) != num_stars:\n",
    "\t\traise ValueError('pmdec_error must be a numpy array of the same size as ra !')\n",
    "\tif rv_error is not None and np.size(rv_error) != num_stars:\n",
    "\t\traise ValueError('rv_error must be a numpy array of the same size as ra !')\n",
    "\tif dist_error is not None and np.size(dist_error) != num_stars:\n",
    "\t\traise ValueError('dist_error must be a numpy array of the same size as ra !')\n",
    "\n",
    "\t#Compute elements of the T matrix\n",
    "\tcos_ra = np.cos(np.radians(ra))\n",
    "\tcos_dec = np.cos(np.radians(dec))\n",
    "\tsin_ra = np.sin(np.radians(ra))\n",
    "\tsin_dec = np.sin(np.radians(dec))\n",
    "\tT1 = TGAL[0,0]*cos_ra*cos_dec + TGAL[0,1]*sin_ra*cos_dec + TGAL[0,2]*sin_dec\n",
    "\tT2 = -TGAL[0,0]*sin_ra + TGAL[0,1]*cos_ra\n",
    "\tT3 = -TGAL[0,0]*cos_ra*sin_dec - TGAL[0,1]*sin_ra*sin_dec + TGAL[0,2]*cos_dec\n",
    "\tT4 = TGAL[1,0]*cos_ra*cos_dec + TGAL[1,1]*sin_ra*cos_dec + TGAL[1,2]*sin_dec\n",
    "\tT5 = -TGAL[1,0]*sin_ra + TGAL[1,1]*cos_ra\n",
    "\tT6 = -TGAL[1,0]*cos_ra*sin_dec - TGAL[1,1]*sin_ra*sin_dec + TGAL[1,2]*cos_dec\n",
    "\tT7 = TGAL[2,0]*cos_ra*cos_dec + TGAL[2,1]*sin_ra*cos_dec + TGAL[2,2]*sin_dec\n",
    "\tT8 = -TGAL[2,0]*sin_ra + TGAL[2,1]*cos_ra\n",
    "\tT9 = -TGAL[2,0]*cos_ra*sin_dec - TGAL[2,1]*sin_ra*sin_dec + TGAL[2,2]*cos_dec\n",
    "\n",
    "\t#Calculate UVW\n",
    "\treduced_dist = kappa*dist\n",
    "\tU = T1*rv + T2*pmra*reduced_dist + T3*pmdec*reduced_dist\n",
    "\tV = T4*rv + T5*pmra*reduced_dist + T6*pmdec*reduced_dist\n",
    "\tW = T7*rv + T8*pmra*reduced_dist + T9*pmdec*reduced_dist\n",
    "\n",
    "\t#Return only (U, V, W) tuple if no errors are set\n",
    "\tif pmra_error is None and pmdec_error is None and rv_error is None and dist_error is None:\n",
    "\t\treturn (U, V, W)\n",
    "\n",
    "\t#Propagate errors if they are specified\n",
    "\tif pmra_error is None:\n",
    "\t\tpmra_error = np.zeros(num_stars)\n",
    "\tif pmdec_error is None:\n",
    "\t\tpmdec_error = np.zeros(num_stars)\n",
    "\tif rv_error is None:\n",
    "\t\trv_error = np.zeros(num_stars)\n",
    "\tif dist_error is None:\n",
    "\t\tdist_error = np.zeros(num_stars)\n",
    "\treduced_dist_error = kappa*dist_error\n",
    "\n",
    "\t#Calculate derivatives\n",
    "\tT23_pm = np.sqrt((T2*pmra)**2+(T3*pmdec)**2)\n",
    "\tT23_pm_error = np.sqrt((T2*pmra_error)**2+(T3*pmdec_error)**2)\n",
    "\tEU_rv = T1 * rv_error\n",
    "\tEU_pm = T23_pm_error * reduced_dist\n",
    "\tEU_dist = T23_pm * reduced_dist_error\n",
    "\tEU_dist_pm = T23_pm_error * reduced_dist_error\n",
    "\n",
    "\tT56_pm = np.sqrt((T5*pmra)**2+(T6*pmdec)**2)\n",
    "\tT56_pm_error = np.sqrt((T5*pmra_error)**2+(T6*pmdec_error)**2)\n",
    "\tEV_rv = T4 * rv_error\n",
    "\tEV_pm = T56_pm_error * reduced_dist\n",
    "\tEV_dist = T56_pm * reduced_dist_error\n",
    "\tEV_dist_pm = T56_pm_error * reduced_dist_error\n",
    "\n",
    "\tT89_pm = np.sqrt((T8*pmra)**2+(T9*pmdec)**2)\n",
    "\tT89_pm_error = np.sqrt((T8*pmra_error)**2+(T9*pmdec_error)**2)\n",
    "\tEW_rv = T7 * rv_error\n",
    "\tEW_pm = T89_pm_error * reduced_dist\n",
    "\tEW_dist = T89_pm * reduced_dist_error\n",
    "\tEW_dist_pm = T89_pm_error * reduced_dist_error\n",
    "\n",
    "\t#Calculate error bars\n",
    "\tEU = np.sqrt(EU_rv**2 + EU_pm**2 + EU_dist**2 + EU_dist_pm**2)\n",
    "\tEV = np.sqrt(EV_rv**2 + EV_pm**2 + EV_dist**2 + EV_dist_pm**2)\n",
    "\tEW = np.sqrt(EW_rv**2 + EW_pm**2 + EW_dist**2 + EW_dist_pm**2)\n",
    "\n",
    "\t#Return measurements and error bars\n",
    "\treturn (U, V, W, EU, EV, EW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
